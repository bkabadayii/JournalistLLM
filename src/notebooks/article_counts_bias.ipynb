{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4b2201",
   "metadata": {},
   "source": [
    "# American Journalists: Byline Counts & Bias Profiles\n",
    "\n",
    "This starter notebook helps you:\n",
    "1. **Ingest _All the News 2.0_** (2.69M U.S. news articles, 2016–2020).\n",
    "2. **Produce per-author (journalist) article counts** from bylines.\n",
    "3. **Attach outlet-level political bias labels** from **AllSides** and **MBFC** to compute simple **per-author bias profiles**.\n",
    "4. **Scrape AllSides _author-level_ ratings** (for journalists who have an AllSides page) and merge with your counts.\n",
    "\n",
    "> **Licenses & Terms**  \n",
    "> • _All the News 2.0_ is for **non-commercial research** only and explicitly **not for training commercial generative models**. Download it from the official page and respect their terms.  \n",
    "> • AllSides and MBFC have their own terms. For MBFC, the recommended free route is using **NELA-GT** (which bundles MBFC source labels) or the research CSV used below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6daabf",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09d540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR = /Users/baturalpkabadayi/Dev/Research/JournalistLLM/data\n",
      "OUT_DIR  = /Users/baturalpkabadayi/Dev/Research/JournalistLLM/outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can run this entire notebook locally. Internet is required for the auto-download helpers.\n",
    "# If a download fails (e.g., behind a firewall), download manually from the provided links\n",
    "# and place files in the `data/` folder defined below.\n",
    "\n",
    "import re, zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "AUTO_DOWNLOAD = True  # set to False to skip any attempted downloads\n",
    "\n",
    "def safe_download(url: str, dest: Path, chunk=1<<20):\n",
    "    \"\"\"Stream-download a file to dest. Overwrites if exists.\"\"\"\n",
    "    import requests\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with requests.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for c in r.iter_content(chunk_size=chunk):\n",
    "                if c:\n",
    "                    f.write(c)\n",
    "    return dest\n",
    "\n",
    "def unzip_if_needed(path: Path, dest_dir: Path):\n",
    "    if path.suffix.lower() == \".zip\":\n",
    "        with zipfile.ZipFile(path, 'r') as zf:\n",
    "            zf.extractall(dest_dir)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Helper to normalize domains (strip subdomains like 'www.')\n",
    "def normalize_domain(url_or_domain: str) -> str:\n",
    "    s = url_or_domain.strip().lower()\n",
    "    s = re.sub(r'^https?://', '', s)\n",
    "    s = s.split('/')[0]\n",
    "    s = re.sub(r'^www\\.', '', s)\n",
    "    return s\n",
    "\n",
    "# All the outputs will be written under 'outputs/'\n",
    "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "print(\"DATA_DIR =\", DATA_DIR.resolve())\n",
    "print(\"OUT_DIR  =\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8480ff3",
   "metadata": {},
   "source": [
    "## 1) Get the Articles: _All the News 2.0_\n",
    "\n",
    "**Source & Docs:**  \n",
    "• Components.one dataset page (official host): see fields and counts; download hosted via Proton Drive.  \n",
    "• Mirrors also exist on third-party sites, but you should prefer the official page for the latest schema.\n",
    "\n",
    "**Expected file(s):**\n",
    "- `all-the-news-2.csv` — a single CSV with ~2.7M rows.\n",
    "\n",
    "> If auto-download fails, **go to the official page** and download manually, then place the CSV under `data/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70da35e",
   "metadata": {},
   "source": [
    "### 1.1. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0914b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a small sample to preview schema...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-09 18:31:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Lee Drutman</td>\n",
       "      <td>We should take concerns about the health of li...</td>\n",
       "      <td>This post is part of Polyarchy, an independent...</td>\n",
       "      <td>https://www.vox.com/polyarchy/2016/12/9/138983...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-07 21:26:46</td>\n",
       "      <td>2016</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Scott Davis</td>\n",
       "      <td>Colts GM Ryan Grigson says Andrew Luck's contr...</td>\n",
       "      <td>The Indianapolis Colts made Andrew Luck the h...</td>\n",
       "      <td>https://www.businessinsider.com/colts-gm-ryan-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business Insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-26 00:00:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump denies report he ordered Mueller fired</td>\n",
       "      <td>DAVOS, Switzerland (Reuters) - U.S. President ...</td>\n",
       "      <td>https://www.reuters.com/article/us-davos-meeti...</td>\n",
       "      <td>Davos</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-27 00:00:00</td>\n",
       "      <td>2019</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France's Sarkozy reveals his 'Passions' but in...</td>\n",
       "      <td>PARIS (Reuters) - Former French president Nico...</td>\n",
       "      <td>https://www.reuters.com/article/france-politic...</td>\n",
       "      <td>World News</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-27 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris Hilton: Woman In Black For Uncle Monty's...</td>\n",
       "      <td>Paris Hilton arrived at LAX Wednesday dressed ...</td>\n",
       "      <td>https://www.tmz.com/2016/01/27/paris-hilton-mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TMZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  year  month  day       author  \\\n",
       "0  2016-12-09 18:31:00  2016   12.0    9  Lee Drutman   \n",
       "1  2016-10-07 21:26:46  2016   10.0    7  Scott Davis   \n",
       "2  2018-01-26 00:00:00  2018    1.0   26          NaN   \n",
       "3  2019-06-27 00:00:00  2019    6.0   27          NaN   \n",
       "4  2016-01-27 00:00:00  2016    1.0   27          NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0  We should take concerns about the health of li...   \n",
       "1  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
       "2       Trump denies report he ordered Mueller fired   \n",
       "3  France's Sarkozy reveals his 'Passions' but in...   \n",
       "4  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
       "\n",
       "                                             article  \\\n",
       "0  This post is part of Polyarchy, an independent...   \n",
       "1   The Indianapolis Colts made Andrew Luck the h...   \n",
       "2  DAVOS, Switzerland (Reuters) - U.S. President ...   \n",
       "3  PARIS (Reuters) - Former French president Nico...   \n",
       "4  Paris Hilton arrived at LAX Wednesday dressed ...   \n",
       "\n",
       "                                                 url     section  \\\n",
       "0  https://www.vox.com/polyarchy/2016/12/9/138983...         NaN   \n",
       "1  https://www.businessinsider.com/colts-gm-ryan-...         NaN   \n",
       "2  https://www.reuters.com/article/us-davos-meeti...       Davos   \n",
       "3  https://www.reuters.com/article/france-politic...  World News   \n",
       "4  https://www.tmz.com/2016/01/27/paris-hilton-mo...         NaN   \n",
       "\n",
       "        publication  \n",
       "0               Vox  \n",
       "1  Business Insider  \n",
       "2           Reuters  \n",
       "3           Reuters  \n",
       "4               TMZ  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Configure download URL(s) ---\n",
    "# Official page: https://components.one/datasets/all-the-news-2-news-articles-dataset\n",
    "# The page links to a Proton Drive URL (changes occasionally). Manual download is recommended.\n",
    "ALL_NEWS_CSV = DATA_DIR / \"all-the-news-2.csv\"  # <-- put your downloaded CSV here\n",
    "\n",
    "if AUTO_DOWNLOAD and not ALL_NEWS_CSV.exists():\n",
    "    print(\"Auto-download is not guaranteed for All the News 2.0 (Proton Drive link).\")\n",
    "    print(\"Please download manually from the official page and place it at:\", ALL_NEWS_CSV.resolve())\n",
    "\n",
    "# Quick schema peek (only runs if file is present)\n",
    "if ALL_NEWS_CSV.exists():\n",
    "    print(\"Loading a small sample to preview schema...\")\n",
    "    sample = pd.read_csv(ALL_NEWS_CSV, nrows=5)\n",
    "    display(sample.head())\n",
    "else:\n",
    "    print(\"CSV not found yet. Proceed after you place the file at:\", ALL_NEWS_CSV.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3770f",
   "metadata": {},
   "source": [
    "### 1.2. Domain Normalization & Duplicate Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daebca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65412463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/w5cgd6hs1tsd7tddgjp26yn80000gn/T/ipykernel_33829/59631261.py:49: DtypeWarning: Columns (4,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(ALL_NEWS_CSV, chunksize=CHUNKSIZE):\n",
      "/var/folders/p4/w5cgd6hs1tsd7tddgjp26yn80000gn/T/ipykernel_33829/59631261.py:49: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(ALL_NEWS_CSV, chunksize=CHUNKSIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - outputs/domain_check_reports/publication_domain_pair_counts.csv\n",
      " - outputs/domain_check_reports/publications_with_multiple_domains.csv\n",
      " - outputs/domain_check_reports/domains_with_multiple_publications.csv\n",
      " - outputs/domain_check_reports/publication_domain_map.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/w5cgd6hs1tsd7tddgjp26yn80000gn/T/ipykernel_33829/59631261.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(top_share_for_pub)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>canonical_domain</th>\n",
       "      <th>top_count</th>\n",
       "      <th>total_pairs</th>\n",
       "      <th>top_share</th>\n",
       "      <th>pub_total_rows</th>\n",
       "      <th>missing_url_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>840094</td>\n",
       "      <td>840094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>840094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNBC</td>\n",
       "      <td>cnbc.com</td>\n",
       "      <td>238096</td>\n",
       "      <td>238096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>238096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>208411</td>\n",
       "      <td>208411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>208411</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>136488</td>\n",
       "      <td>136488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136488</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>127602</td>\n",
       "      <td>127602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127602</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Refinery 29</td>\n",
       "      <td>refinery29.com</td>\n",
       "      <td>111433</td>\n",
       "      <td>111433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111433</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vice</td>\n",
       "      <td>vice.com</td>\n",
       "      <td>101137</td>\n",
       "      <td>101137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mashable</td>\n",
       "      <td>mashable.com</td>\n",
       "      <td>94107</td>\n",
       "      <td>94107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94107</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>businessinsider.com</td>\n",
       "      <td>57953</td>\n",
       "      <td>57953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57953</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>52424</td>\n",
       "      <td>52424</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52424</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>52095</td>\n",
       "      <td>52095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TMZ</td>\n",
       "      <td>tmz.com</td>\n",
       "      <td>49595</td>\n",
       "      <td>49595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49595</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publication     canonical_domain  top_count  total_pairs  top_share  \\\n",
       "0            Reuters          reuters.com     840094       840094        1.0   \n",
       "1               CNBC             cnbc.com     238096       238096        1.0   \n",
       "2           The Hill          thehill.com     208411       208411        1.0   \n",
       "3             People           people.com     136488       136488        1.0   \n",
       "4                CNN              cnn.com     127602       127602        1.0   \n",
       "5        Refinery 29       refinery29.com     111433       111433        1.0   \n",
       "6               Vice             vice.com     101137       101137        1.0   \n",
       "7           Mashable         mashable.com      94107        94107        1.0   \n",
       "8   Business Insider  businessinsider.com      57953        57953        1.0   \n",
       "9          The Verge         theverge.com      52424        52424        1.0   \n",
       "10        TechCrunch       techcrunch.com      52095        52095        1.0   \n",
       "11               TMZ              tmz.com      49595        49595        1.0   \n",
       "\n",
       "    pub_total_rows  missing_url_rows  \n",
       "0           840094               0.0  \n",
       "1           238096               0.0  \n",
       "2           208411               0.0  \n",
       "3           136488               0.0  \n",
       "4           127602               0.0  \n",
       "5           111433               0.0  \n",
       "6           101137               0.0  \n",
       "7            94107               0.0  \n",
       "8            57953               0.0  \n",
       "9            52424               0.0  \n",
       "10           52095               0.0  \n",
       "11           49595               0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>distinct_domains</th>\n",
       "      <th>top_domain</th>\n",
       "      <th>top_count</th>\n",
       "      <th>total_pairs</th>\n",
       "      <th>top_share</th>\n",
       "      <th>pub_total_rows</th>\n",
       "      <th>missing_url_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>26</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>247587</td>\n",
       "      <td>252259</td>\n",
       "      <td>0.981479</td>\n",
       "      <td>252259</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gizmodo</td>\n",
       "      <td>3</td>\n",
       "      <td>gizmodo.com</td>\n",
       "      <td>27225</td>\n",
       "      <td>27228</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>27228</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          publication  distinct_domains   top_domain  top_count  total_pairs  \\\n",
       "0  The New York Times                26  nytimes.com     247587       252259   \n",
       "1             Gizmodo                 3  gizmodo.com      27225        27228   \n",
       "\n",
       "   top_share  pub_total_rows  missing_url_rows  \n",
       "0   0.981479          252259               0.0  \n",
       "1   0.999890           27228               0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) DOMAIN NORMALIZATION & DUPLICATE CHECKS\n",
    "# =========================\n",
    "try:\n",
    "    ALL_NEWS_CSV\n",
    "except NameError:\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    ALL_NEWS_CSV = DATA_DIR / \"all-the-news-2.csv\"\n",
    "\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "report_out = OUT_DIR / \"domain_check_reports\"\n",
    "\n",
    "try:\n",
    "    report_out.mkdir(exist_ok=True)\n",
    "except NameError:\n",
    "    report_out = Path(\"outputs/domain_check_reports\")\n",
    "    report_out.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Robust domain normalizer (safe on NaN / empty / malformed URLs)\n",
    "def normalize_domain(url_or_domain: str) -> str | None:\n",
    "    if pd.isna(url_or_domain):\n",
    "        return None\n",
    "    s = str(url_or_domain).strip().lower()\n",
    "    if not s:\n",
    "        return None\n",
    "    # strip scheme\n",
    "    s = re.sub(r'^https?://', '', s)\n",
    "    # take netloc\n",
    "    s = s.split('/')[0]\n",
    "    # strip www.\n",
    "    s = re.sub(r'^www\\.', '', s)\n",
    "    return s or None\n",
    "\n",
    "# --- Configure chunking\n",
    "CHUNKSIZE = 200_000\n",
    "\n",
    "# --- Accumulators\n",
    "pair_counts = defaultdict(int)        # (publication, normalized_domain) -> count\n",
    "pub_missing_url = defaultdict(int)    # publication -> rows with missing/invalid url\n",
    "domain_counts = defaultdict(int)      # normalized_domain -> total count\n",
    "pub_counts = defaultdict(int)         # publication -> total rows seen (with or without URL)\n",
    "\n",
    "# --- First pass: compute counts for every (publication, normalized_domain)\n",
    "for chunk in pd.read_csv(ALL_NEWS_CSV, chunksize=CHUNKSIZE):\n",
    "    # Keep only needed columns\n",
    "    sub = chunk[['publication', 'url']].copy()\n",
    "    # Normalize\n",
    "    sub['publication'] = sub['publication'].astype(str).str.strip()\n",
    "    sub['normalized_domain'] = sub['url'].map(normalize_domain)\n",
    "\n",
    "    # Tally totals per publication\n",
    "    for p, n in sub['publication'].value_counts().items():\n",
    "        pub_counts[p] += int(n)\n",
    "\n",
    "    # Missing/invalid URL rows\n",
    "    miss = sub[sub['normalized_domain'].isna()]\n",
    "    for p, n in miss['publication'].value_counts().items():\n",
    "        pub_missing_url[p] += int(n)\n",
    "\n",
    "    # Valid domain rows: tally (publication, domain) and per-domain totals\n",
    "    ok = sub.dropna(subset=['normalized_domain'])\n",
    "    gb = ok.groupby(['publication', 'normalized_domain'], as_index=False).size()\n",
    "    for _, row in gb.iterrows():\n",
    "        key = (row['publication'], row['normalized_domain'])\n",
    "        pair_counts[key] += int(row['size'])\n",
    "        domain_counts[row['normalized_domain']] += int(row['size'])\n",
    "\n",
    "# --- Build DataFrames from accumulators\n",
    "pairs_df = (\n",
    "    pd.DataFrame(\n",
    "        [(p, d, c) for (p, d), c in pair_counts.items()],\n",
    "        columns=['publication', 'normalized_domain', 'pair_count']\n",
    "    )\n",
    "    .sort_values(['publication', 'pair_count'], ascending=[True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "pub_totals = (\n",
    "    pd.DataFrame(list(pub_counts.items()), columns=['publication', 'pub_total_rows'])\n",
    "    .merge(\n",
    "        pd.DataFrame(list(pub_missing_url.items()), columns=['publication', 'missing_url_rows']),\n",
    "        on='publication', how='left'\n",
    "    )\n",
    "    .fillna({'missing_url_rows': 0})\n",
    ")\n",
    "\n",
    "domain_totals = pd.DataFrame(list(domain_counts.items()), columns=['normalized_domain', 'domain_total_rows'])\n",
    "\n",
    "# --- Publications that map to >1 normalized domain\n",
    "multi_domain = (\n",
    "    pairs_df.groupby('publication')['normalized_domain']\n",
    "    .nunique()\n",
    "    .reset_index(name='distinct_domains')\n",
    ")\n",
    "multi_domain = multi_domain[multi_domain['distinct_domains'] > 1]\n",
    "\n",
    "# For those, compute coverage ratio of the top domain for that publication\n",
    "def top_share_for_pub(pdf: pd.DataFrame) -> pd.Series:\n",
    "    pdf = pdf.sort_values('pair_count', ascending=False)\n",
    "    top_domain = pdf.iloc[0]['normalized_domain']\n",
    "    top_count = pdf.iloc[0]['pair_count']\n",
    "    total = int(pdf['pair_count'].sum())\n",
    "    return pd.Series({\n",
    "        'top_domain': top_domain,\n",
    "        'top_count': int(top_count),\n",
    "        'total_pairs': total,\n",
    "        'top_share': float(top_count) / total if total else np.nan\n",
    "    })\n",
    "\n",
    "coverage = (\n",
    "    pairs_df\n",
    "    .groupby('publication', group_keys=False)\n",
    "    .apply(top_share_for_pub)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "publications_with_multiple_domains = (\n",
    "    multi_domain\n",
    "    .merge(coverage, on='publication', how='left')\n",
    "    .merge(pub_totals, on='publication', how='left')\n",
    "    .sort_values(['distinct_domains', 'top_share'], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Domains used by >1 publication\n",
    "domains_with_multiple_publications = (\n",
    "    pairs_df.groupby('normalized_domain')['publication']\n",
    "    .nunique()\n",
    "    .reset_index(name='distinct_publications')\n",
    ")\n",
    "domains_with_multiple_publications = domains_with_multiple_publications[\n",
    "    domains_with_multiple_publications['distinct_publications'] > 1\n",
    "].sort_values('distinct_publications', ascending=False)\n",
    "\n",
    "# --- Canonical mapping: pick the most frequent domain per publication\n",
    "#     (You can later filter by a minimum top_share if you want to flag ambiguous cases)\n",
    "canonical_map = (\n",
    "    pairs_df.sort_values(['publication', 'pair_count'], ascending=[True, False])\n",
    "    .groupby('publication', as_index=False)\n",
    "    .first()[['publication', 'normalized_domain']]\n",
    "    .rename(columns={'normalized_domain': 'canonical_domain'})\n",
    ")\n",
    "canonical_map = (\n",
    "    canonical_map\n",
    "    .merge(coverage[['publication', 'top_count', 'total_pairs', 'top_share']], on='publication', how='left')\n",
    "    .merge(pub_totals, on='publication', how='left')\n",
    "    .sort_values(['top_share', 'top_count'], ascending=[False, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Write reports\n",
    "pairs_df.to_csv(report_out / \"publication_domain_pair_counts.csv\", index=False)\n",
    "publications_with_multiple_domains.to_csv(report_out / \"publications_with_multiple_domains.csv\", index=False)\n",
    "domains_with_multiple_publications.to_csv(report_out / \"domains_with_multiple_publications.csv\", index=False)\n",
    "canonical_map.to_csv(report_out / \"publication_domain_map.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", report_out / \"publication_domain_pair_counts.csv\")\n",
    "print(\" -\", report_out / \"publications_with_multiple_domains.csv\")\n",
    "print(\" -\", report_out / \"domains_with_multiple_publications.csv\")\n",
    "print(\" -\", report_out / \"publication_domain_map.csv\")\n",
    "\n",
    "# --- Quick preview\n",
    "display(canonical_map.head(12))\n",
    "display(publications_with_multiple_domains.head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac989d6",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Per-Author Article Counts\n",
    "\n",
    "We parse the `author` field and count bylines per journalist.  \n",
    "Notes:\n",
    "- Multi-bylines like `\"A, B\"` are split on commas.\n",
    "- Empty or missing authors are dropped (you can keep them if needed).\n",
    "- We'll also keep **outlet counts** per author and the **time span** of their articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27401181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) BUILD AUTHOR x OUTLET x DOMAIN COUNTS (CHUNKED)\n",
    "# =========================\n",
    "\n",
    "# --- Inputs\n",
    "try:\n",
    "    ALL_NEWS_CSV\n",
    "except NameError:\n",
    "    DATA_DIR = Path(\"data\"); DATA_DIR.mkdir(exist_ok=True)\n",
    "    ALL_NEWS_CSV = DATA_DIR / \"all-the-news-2.csv\"\n",
    "\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CANONICAL_MAP_CSV = report_out / \"publication_domain_map.csv\"\n",
    "assert CANONICAL_MAP_CSV.exists(), \"Run Section 1 to generate publication_domain_map.csv first.\"\n",
    "\n",
    "# --- Load canonical mapping {publication -> canonical_domain}\n",
    "pub_map = pd.read_csv(CANONICAL_MAP_CSV)[['publication', 'canonical_domain']].copy()\n",
    "pub_map['publication'] = pub_map['publication'].astype(str).str.strip()\n",
    "\n",
    "# --- Normalizer (redefine here for isolation)\n",
    "def normalize_domain(url_or_domain: str) -> str | None:\n",
    "    if pd.isna(url_or_domain):\n",
    "        return None\n",
    "    s = str(url_or_domain).strip().lower()\n",
    "    if not s:\n",
    "        return None\n",
    "    s = re.sub(r'^https?://', '', s)\n",
    "    s = s.split('/')[0]\n",
    "    s = re.sub(r'^www\\.', '', s)\n",
    "    return s or None\n",
    "\n",
    "def explode_authors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split comma-separated bylines into one row per author.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['author'] = df['author'].fillna('').astype(str)\n",
    "    df = df[df['author'].str.strip() != '']\n",
    "    df['author'] = df['author'].str.split(',')\n",
    "    df = df.explode('author')\n",
    "    df['author'] = df['author'].str.strip()\n",
    "    df = df[df['author'] != '']\n",
    "    return df\n",
    "\n",
    "# --- Person-like heuristics (tweak as you like)\n",
    "PERSON_LIKE_RULES = {\n",
    "    \"min_alpha_len\": 3,\n",
    "    \"require_space\": True,\n",
    "    \"blocklist_substrings\": [\n",
    "        \"staff\", \"editorial\", \"opinion\", \"desk\", \"team\", \"associated press\",\n",
    "        \"reuters\", \"field level media\", \"cnn\", \"ap\", \"buzzfeed news\",\n",
    "        \"wired staff\", \"bbc\", \"npr\", \"nyt cooking\", \"vox staff\"\n",
    "    ],\n",
    "    \"all_caps_threshold\": 0.8,\n",
    "}\n",
    "def looks_person_like(name: str) -> bool:\n",
    "    n = str(name).strip()\n",
    "    if not n:\n",
    "        return False\n",
    "    low = n.lower()\n",
    "    if any(s in low for s in PERSON_LIKE_RULES[\"blocklist_substrings\"]):\n",
    "        return False\n",
    "    if PERSON_LIKE_RULES[\"require_space\"] and \" \" not in n:\n",
    "        return False\n",
    "    letters = re.findall(r\"[A-Za-z]\", n)\n",
    "    if len(letters) < PERSON_LIKE_RULES[\"min_alpha_len\"]:\n",
    "        return False\n",
    "    if letters:\n",
    "        caps = sum(1 for ch in letters if ch.isupper())\n",
    "        if caps / len(letters) >= PERSON_LIKE_RULES[\"all_caps_threshold\"]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# --- Accumulate counts per (author, publication, domain)\n",
    "CHUNKSIZE = 250_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - outputs/author_outlet_counts.csv\n",
      " - outputs/author_outlet_counts_humanlike.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>domain</th>\n",
       "      <th>article_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>24477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opinion contributor</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>19551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WIRED Staff</td>\n",
       "      <td>Wired</td>\n",
       "      <td>wired.com</td>\n",
       "      <td>15815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Field Level Media</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>7890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Bowden</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>7023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Associated Press</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>6797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rebecca Savransky</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>5649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Julia Manchester</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>5499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brett Samuels</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>People Staff</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>5393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Axios</td>\n",
       "      <td>Axios</td>\n",
       "      <td>axios.com</td>\n",
       "      <td>5276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Max Greenwood</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dave Quinn</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>5023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author         publication       domain  article_count\n",
       "0                    CNN                 CNN      cnn.com          24477\n",
       "1    opinion contributor            The Hill  thehill.com          19551\n",
       "2            WIRED Staff               Wired    wired.com          15815\n",
       "3      Field Level Media             Reuters  reuters.com           7890\n",
       "4            John Bowden            The Hill  thehill.com           7023\n",
       "5       Associated Press            Fox News  foxnews.com           6797\n",
       "6      Rebecca Savransky            The Hill  thehill.com           5944\n",
       "7   The Associated Press  The New York Times  nytimes.com           5649\n",
       "8       Julia Manchester            The Hill  thehill.com           5503\n",
       "9     The New York Times  The New York Times  nytimes.com           5499\n",
       "10         Brett Samuels            The Hill  thehill.com           5395\n",
       "11          People Staff              People   people.com           5393\n",
       "12                 Axios               Axios    axios.com           5276\n",
       "13         Max Greenwood            The Hill  thehill.com           5151\n",
       "14            Dave Quinn              People   people.com           5023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>domain</th>\n",
       "      <th>article_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bowden</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>7023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Savransky</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Julia Manchester</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>5499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brett Samuels</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Max Greenwood</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave Quinn</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>5023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Michael D. Shear</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>4689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jordain Carney</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>4461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexia Fernandez</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stephanie Petit</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Karen Mizoguchi</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maggie Astor</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>4155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nick Corasaniti</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>3789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author         publication          domain  article_count\n",
       "0          John Bowden            The Hill     thehill.com           7023\n",
       "1    Rebecca Savransky            The Hill     thehill.com           5944\n",
       "2     Julia Manchester            The Hill     thehill.com           5503\n",
       "3   The New York Times  The New York Times     nytimes.com           5499\n",
       "4        Brett Samuels            The Hill     thehill.com           5395\n",
       "5        Max Greenwood            The Hill     thehill.com           5151\n",
       "6           Dave Quinn              People      people.com           5023\n",
       "7     Michael D. Shear  The New York Times     nytimes.com           4689\n",
       "8       Jordain Carney            The Hill     thehill.com           4461\n",
       "9     Alexia Fernandez              People      people.com           4326\n",
       "10     Stephanie Petit              People      people.com           4191\n",
       "11     Karen Mizoguchi              People      people.com           4172\n",
       "12        Maggie Astor  The New York Times     nytimes.com           4155\n",
       "13     Nick Corasaniti  The New York Times     nytimes.com           3789\n",
       "14         Sarah Perez          TechCrunch  techcrunch.com           3749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accum = []\n",
    "\n",
    "for chunk in pd.read_csv(ALL_NEWS_CSV, chunksize=CHUNKSIZE):\n",
    "    sub = chunk[['author', 'publication', 'url']].copy()\n",
    "    sub['publication'] = sub['publication'].astype(str).str.strip()\n",
    "\n",
    "    # Raw normalized domain from URL\n",
    "    sub['normalized_domain'] = sub['url'].map(normalize_domain)\n",
    "\n",
    "    # Attach canonical domain by publication, fall back to raw normalized_domain when missing\n",
    "    sub = sub.merge(pub_map, on='publication', how='left')\n",
    "    sub['domain'] = sub['canonical_domain'].fillna(sub['normalized_domain'])\n",
    "\n",
    "    # We can safely drop rows with missing domain if we will join to MBFC/AllSides later\n",
    "    sub = sub.dropna(subset=['domain'])\n",
    "\n",
    "    # Split multi-byline authors\n",
    "    sub = explode_authors(sub)\n",
    "\n",
    "    # Per-chunk groupby\n",
    "    g = (sub\n",
    "         .groupby(['author', 'publication', 'domain'], as_index=False)\n",
    "         .size()\n",
    "         .rename(columns={'size': 'article_count'}))\n",
    "    accum.append(g)\n",
    "\n",
    "# Merge all chunk results\n",
    "author_outlet_counts = (\n",
    "    pd.concat(accum, ignore_index=True)\n",
    "    .groupby(['author', 'publication', 'domain'], as_index=False)['article_count']\n",
    "    .sum()\n",
    "    .sort_values('article_count', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Humanlike filtered version\n",
    "author_outlet_counts_humanlike = author_outlet_counts[\n",
    "    author_outlet_counts['author'].map(looks_person_like)\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "# --- Save\n",
    "author_outlet_counts_filename = \"author_outlet_counts.csv\"\n",
    "author_outlet_counts.to_csv(OUT_DIR / author_outlet_counts_filename, index=False)\n",
    "\n",
    "author_outlet_counts_humanlike_filename = \"author_outlet_counts_humanlike.csv\"\n",
    "author_outlet_counts_humanlike.to_csv(OUT_DIR / author_outlet_counts_humanlike_filename, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", OUT_DIR / author_outlet_counts_filename)\n",
    "print(\" -\", OUT_DIR / author_outlet_counts_humanlike_filename)\n",
    "\n",
    "# --- Quick peek\n",
    "display(author_outlet_counts.head(15))\n",
    "display(author_outlet_counts_humanlike.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55b037",
   "metadata": {},
   "source": [
    "### 2.1. Split Multiple Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47caa552",
   "metadata": {},
   "source": [
    "This reads a chosen counts file, splits author cells containing ```\" and \"``` into separate rows, then re-aggregates to keep one row per ```(author, publication, domain)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6825c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (overwritten): outputs/author_outlet_counts_humanlike.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>domain</th>\n",
       "      <th>article_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bowden</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>7085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Savransky</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brett Samuels</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julia Manchester</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>5499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Max Greenwood</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave Quinn</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>5023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jordain Carney</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>4846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Michael D. Shear</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>4691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexia Fernandez</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stephanie Petit</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Karen Mizoguchi</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maggie Astor</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>4155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nick Corasaniti</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>3789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author         publication          domain  article_count\n",
       "0          John Bowden            The Hill     thehill.com           7085\n",
       "1    Rebecca Savransky            The Hill     thehill.com           5996\n",
       "2        Brett Samuels            The Hill     thehill.com           5732\n",
       "3     Julia Manchester            The Hill     thehill.com           5578\n",
       "4   The New York Times  The New York Times     nytimes.com           5499\n",
       "5        Max Greenwood            The Hill     thehill.com           5361\n",
       "6           Dave Quinn              People      people.com           5023\n",
       "7       Jordain Carney            The Hill     thehill.com           4846\n",
       "8     Michael D. Shear  The New York Times     nytimes.com           4691\n",
       "9     Alexia Fernandez              People      people.com           4326\n",
       "10     Stephanie Petit              People      people.com           4191\n",
       "11     Karen Mizoguchi              People      people.com           4172\n",
       "12        Maggie Astor  The New York Times     nytimes.com           4155\n",
       "13     Nick Corasaniti  The New York Times     nytimes.com           3789\n",
       "14         Sarah Perez          TechCrunch  techcrunch.com           3749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Choose which file to fix\n",
    "author_outlet_counts_filename = \"author_outlet_counts_humanlike.csv\"\n",
    "\n",
    "# ---- Paths (fallbacks if these weren't defined earlier)\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "in_path = OUT_DIR / author_outlet_counts_filename\n",
    "\n",
    "# ---- Load\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "# ---- Sanity: required columns\n",
    "required_cols = {\"author\", \"publication\", \"domain\", \"article_count\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "\n",
    "# ---- Split function: only split on ' and ' (case-insensitive), leave other separators alone\n",
    "AND_PAT = re.compile(r\"\\s+and\\s+\", flags=re.IGNORECASE)\n",
    "\n",
    "def split_and_expand(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "    # Only attempt split where there's no comma (commas were already exploded earlier)\n",
    "    mask = f[\"author\"].astype(str).str.contains(AND_PAT) & ~f[\"author\"].astype(str).str.contains(\",\")\n",
    "    f.loc[mask, \"author\"] = f.loc[mask, \"author\"].astype(str).str.split(AND_PAT)\n",
    "\n",
    "    # Everything else becomes a 1-element list to allow explode\n",
    "    f.loc[~mask, \"author\"] = f.loc[~mask, \"author\"].astype(str).apply(lambda x: [x])\n",
    "\n",
    "    # Explode and trim\n",
    "    f = f.explode(\"author\")\n",
    "    f[\"author\"] = f[\"author\"].astype(str).str.strip()\n",
    "    # Drop empties if any\n",
    "    f = f[f[\"author\"] != \"\"]\n",
    "    return f\n",
    "\n",
    "df_expanded = split_and_expand(df)\n",
    "\n",
    "# ---- Re-aggregate to merge duplicate rows created by the split\n",
    "collapsed = (\n",
    "    df_expanded\n",
    "    .groupby([\"author\", \"publication\", \"domain\"], as_index=False)[\"article_count\"]\n",
    "    .sum()\n",
    "    .sort_values(\"article_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---- Save back to the SAME file (overwrite)\n",
    "collapsed.to_csv(in_path, index=False)\n",
    "print(f\"Saved (overwritten): {in_path}\")\n",
    "display(collapsed.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb789be",
   "metadata": {},
   "source": [
    "### 2.2. Rename Publications and Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28b5c0",
   "metadata": {},
   "source": [
    "This lets us rename publications (e.g., ```\"Vice News\" → \"Vice\"```) to match label sources, then collapses duplicates created by the rename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be150896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (overwritten): outputs/author_outlet_counts_humanlike.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>domain</th>\n",
       "      <th>article_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bowden</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>7085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Savransky</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brett Samuels</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julia Manchester</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>5499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Max Greenwood</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>5361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave Quinn</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>5023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jordain Carney</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>4846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Michael D. Shear</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>4691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexia Fernandez</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stephanie Petit</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Karen Mizoguchi</td>\n",
       "      <td>People</td>\n",
       "      <td>people.com</td>\n",
       "      <td>4172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maggie Astor</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>4155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nick Corasaniti</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>3789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author     publication          domain  article_count\n",
       "0          John Bowden        The Hill     thehill.com           7085\n",
       "1    Rebecca Savransky        The Hill     thehill.com           5996\n",
       "2        Brett Samuels        The Hill     thehill.com           5732\n",
       "3     Julia Manchester        The Hill     thehill.com           5578\n",
       "4   The New York Times  New York Times     nytimes.com           5499\n",
       "5        Max Greenwood        The Hill     thehill.com           5361\n",
       "6           Dave Quinn          People      people.com           5023\n",
       "7       Jordain Carney        The Hill     thehill.com           4846\n",
       "8     Michael D. Shear  New York Times     nytimes.com           4691\n",
       "9     Alexia Fernandez          People      people.com           4326\n",
       "10     Stephanie Petit          People      people.com           4191\n",
       "11     Karen Mizoguchi          People      people.com           4172\n",
       "12        Maggie Astor  New York Times     nytimes.com           4155\n",
       "13     Nick Corasaniti  New York Times     nytimes.com           3789\n",
       "14         Sarah Perez      TechCrunch  techcrunch.com           3749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Choose which file to update\n",
    "author_outlet_counts_filename = \"author_outlet_counts_humanlike.csv\"\n",
    "# author_outlet_counts_filename = \"author_outlet_counts_humanlike.csv\"\n",
    "\n",
    "# ---- Paths (fallbacks if these weren't defined earlier)\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "path = OUT_DIR / author_outlet_counts_filename\n",
    "\n",
    "# ---- Load\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# ---- Renaming dictionary\n",
    "PUB_RENAME = {\n",
    "    \"Vice News\": \"Vice\",\n",
    "    \"The New York Times\": \"New York Times\"\n",
    "}\n",
    "\n",
    "# ---- Apply renaming (exact match)\n",
    "df[\"publication\"] = df[\"publication\"].astype(str).str.strip().replace(PUB_RENAME)\n",
    "\n",
    "# ---- Re-aggregate to merge rows that became identical after renaming\n",
    "df_merged = (\n",
    "    df.groupby([\"author\", \"publication\", \"domain\"], as_index=False)[\"article_count\"]\n",
    "      .sum()\n",
    "      .sort_values(\"article_count\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---- Save back to the SAME file (overwrite)\n",
    "df_merged.to_csv(path, index=False)\n",
    "print(f\"Saved (overwritten): {path}\")\n",
    "display(df_merged.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4440b94",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Outlet Bias (AllSides)\n",
    "\n",
    "We'll use a **public CSV mirror** of the AllSides ratings (as provided by the open-source _AllSideR_ project).  \n",
    "- URL (raw CSV): `https://raw.githubusercontent.com/favstats/AllSideR/master/data/allsides_data.csv`  \n",
    "- Fields include `news_source`, `rating` (Left, Lean Left, Center, Lean Right, Right), and a numeric `rating_num`.\n",
    "\n",
    "> **Note:** AllSides' official methodology includes both categorical ratings and a numeric **Media Bias Meter** in [-6, +6]. This CSV mirrors the categorical form and provides a simple numeric encoding (1..5). You can map to the [-6..+6] scale if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8374153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AllSides ratings...\n"
     ]
    }
   ],
   "source": [
    "ALLSIDES_RAW = DATA_DIR / \"allsides_data.csv\"\n",
    "ALLSIDES_URL = \"https://raw.githubusercontent.com/favstats/AllSideR/master/data/allsides_data.csv\"\n",
    "\n",
    "if AUTO_DOWNLOAD:\n",
    "    try:\n",
    "        print(\"Downloading AllSides ratings...\")\n",
    "        safe_download(ALLSIDES_URL, ALLSIDES_RAW)\n",
    "    except Exception as e:\n",
    "        print(\"AllSides download failed:\", e)\n",
    "        print(\"Download manually from:\", ALLSIDES_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0137e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/allsides_outlet_ratings.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "allsides = None\n",
    "if ALLSIDES_RAW.exists():\n",
    "    allsides = pd.read_csv(ALLSIDES_RAW)\n",
    "    # Keep only news/media outlets (drop 'Author'/'Think tank' etc. if desired)\n",
    "    allsides = allsides[allsides['type'].str.contains(\"News|Media\", case=False, na=False)].copy()\n",
    "    # Standardize columns\n",
    "    allsides = allsides.rename(columns={\n",
    "        \"news_source\": \"source_name\",\n",
    "        \"rating\": \"allsides_label\",\n",
    "        \"rating_num\": \"allsides_num\"\n",
    "    })\n",
    "    # Derive a domain if URL is available\n",
    "    if \"url\" in allsides.columns:\n",
    "        # The URL here actually points to the AllSides *profile* page, not the outlet.\n",
    "        # We'll keep domain empty; we join outlets via a separate publication→domain map.\n",
    "        allsides['domain'] = \"\"\n",
    "    else:\n",
    "        allsides['domain'] = \"\"\n",
    "    # Basic cleanup\n",
    "    allsides['allsides_label'] = allsides['allsides_label'].str.lower()\n",
    "    # Optional: map to [-2..+2] (Left=-2, Lean Left=-1, Center=0, Lean Right=+1, Right=+2)\n",
    "    label_to_score = {\n",
    "        \"left\": -2, \"left-center\": -1, \"lean left\": -1, \"center\": 0,\n",
    "        \"right-center\": 1, \"lean right\": 1, \"right\": 2, \"least biased\": 0\n",
    "    }\n",
    "    allsides['allsides_score5'] = allsides['allsides_label'].map(label_to_score)\n",
    "    allsides.to_csv(\"outputs/allsides_outlet_ratings.csv\", index=False)\n",
    "    print(\"Saved:\", \"outputs/allsides_outlet_ratings.csv\")\n",
    "else:\n",
    "    print(\"AllSides CSV not found. Place it at:\", ALLSIDES_RAW.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dba74",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Outlet Bias (MBFC)\n",
    "\n",
    "There is no official free bulk CSV, but two pragmatic options exist for research:\n",
    "\n",
    "1. **Use a research dataset that standardizes MBFC labels** — e.g., Idiap's open dataset exposes `data/mbfc.csv` (normalized) and `data/mbfc_raw.csv` (scraped), with bias labels like `left`, `left-center`, `center/least biased`, `right-center`, `right`, plus **factual reporting** (`low`, `mixed`, `high`).  \n",
    "   - Raw CSV (normalized): `https://raw.githubusercontent.com/idiap/Factual-Reporting-and-Political-Bias-Web-Interactions/main/data/mbfc.csv`\n",
    "\n",
    "2. **Use NELA-GT (2018/2020/2022)** — the dataset bundles **MBFC source-level labels** you can join by outlet/domain. Download the JSON/SQLite release from its Dataverse DOI (see links in the paper).\n",
    "\n",
    "Below we implement Option 1 for convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd70f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MBFC (normalized) from Idiap...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MBFC_RAW = DATA_DIR / \"mbfc.csv\"\n",
    "MBFC_URL = \"https://raw.githubusercontent.com/idiap/Factual-Reporting-and-Political-Bias-Web-Interactions/main/data/mbfc.csv\"\n",
    "\n",
    "if AUTO_DOWNLOAD:\n",
    "    try:\n",
    "        print(\"Downloading MBFC (normalized) from Idiap...\")\n",
    "        safe_download(MBFC_URL, MBFC_RAW)\n",
    "    except Exception as e:\n",
    "        print(\"MBFC download failed:\", e)\n",
    "        print(\"Manual URL:\", MBFC_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de30cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/mbfc_outlet_ratings.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mbfc = None\n",
    "if MBFC_RAW.exists():\n",
    "    mbfc = pd.read_csv(MBFC_RAW)\n",
    "    # Expected columns: 'source' (domain), 'bias' (e.g., left, left-center, neutral/least-biased, right-center, right),\n",
    "    # 'factual_reporting' (low/mixed/high) – actual names may vary slightly.\n",
    "    mbfc.columns = [c.lower() for c in mbfc.columns]\n",
    "    rename_map = {}\n",
    "    if 'source' in mbfc.columns: rename_map['source'] = 'domain'\n",
    "    if 'bias' in mbfc.columns: rename_map['bias'] = 'mbfc_label'\n",
    "    if 'factual_reporting' in mbfc.columns: rename_map['factual_reporting'] = 'mbfc_factuality'\n",
    "    mbfc = mbfc.rename(columns=rename_map)\n",
    "    # normalize values\n",
    "    def normalize_domain(url_or_domain: str) -> str:\n",
    "        import re\n",
    "        s = str(url_or_domain).strip().lower()\n",
    "        s = re.sub(r'^https?://', '', s)\n",
    "        s = s.split('/')[0]\n",
    "        s = re.sub(r'^www\\.', '', s)\n",
    "        return s\n",
    "    mbfc['domain'] = mbfc['domain'].astype(str).map(normalize_domain)\n",
    "    mbfc['mbfc_label'] = mbfc['mbfc_label'].astype(str).str.lower().str.replace('least biased','center')\n",
    "    # map to symmetric numeric scale (-3..+3, with extremes)\n",
    "    label_to_score7 = {\n",
    "        \"extreme left\": -3, \"far left\": -3, \"left\": -2, \"left-center\": -1, \"center\": 0,\n",
    "        \"right-center\": 1, \"right\": 2, \"extreme right\": 3, \"far right\": 3, \"neutral\": 0, \"least biased\": 0\n",
    "    }\n",
    "    mbfc['mbfc_score7'] = mbfc['mbfc_label'].map(label_to_score7)\n",
    "    mbfc.to_csv(\"outputs/mbfc_outlet_ratings.csv\", index=False)\n",
    "    print(\"Saved:\", \"outputs/mbfc_outlet_ratings.csv\")\n",
    "else:\n",
    "    print(\"MBFC CSV not found. Place it at:\", MBFC_RAW.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f70e1c",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Join Outlet Ratings → Articles → Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2920ec",
   "metadata": {},
   "source": [
    "### 5.1. AllSidesR\n",
    "This joins AllSidesR outlet ratings to the ```(author, publication, domain, article_count)``` table, computes each author’s article-weighted mean AllSides score, plus a few helpful fields:\n",
    "\n",
    "- ```total_articles```, ```main_outlet```, ```main_outlet_share```\n",
    "- ```as_mean_score``` (article-weighted mean of AllSides score in [-2..+2])\n",
    "- optional label counts per author (how many of their articles came from outlets with each AllSides label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d1c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/per_author_allsides_weighted.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>main_outlet</th>\n",
       "      <th>main_outlet_articles</th>\n",
       "      <th>main_outlet_share</th>\n",
       "      <th>as_mean_score</th>\n",
       "      <th>as_articles@center</th>\n",
       "      <th>as_articles@left</th>\n",
       "      <th>as_articles@left-center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bowden</td>\n",
       "      <td>7085</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>7085</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7085.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Savransky</td>\n",
       "      <td>5996</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5996</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brett Samuels</td>\n",
       "      <td>5732</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5732</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5732.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julia Manchester</td>\n",
       "      <td>5588</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5578</td>\n",
       "      <td>0.99821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>5499</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>5499</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Max Greenwood</td>\n",
       "      <td>5361</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5361</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5361.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave Quinn</td>\n",
       "      <td>5023</td>\n",
       "      <td>People</td>\n",
       "      <td>5023</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jordain Carney</td>\n",
       "      <td>4846</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>4846</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4846.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Michael D. Shear</td>\n",
       "      <td>4691</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>4691</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexia Fernandez</td>\n",
       "      <td>4326</td>\n",
       "      <td>People</td>\n",
       "      <td>4326</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stephanie Petit</td>\n",
       "      <td>4191</td>\n",
       "      <td>People</td>\n",
       "      <td>4191</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Karen Mizoguchi</td>\n",
       "      <td>4172</td>\n",
       "      <td>People</td>\n",
       "      <td>4172</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maggie Astor</td>\n",
       "      <td>4155</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>4155</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nick Corasaniti</td>\n",
       "      <td>3789</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>3789</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>3749</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>3749</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3749.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  total_articles     main_outlet  main_outlet_articles  \\\n",
       "0          John Bowden            7085        The Hill                  7085   \n",
       "1    Rebecca Savransky            5996        The Hill                  5996   \n",
       "2        Brett Samuels            5732        The Hill                  5732   \n",
       "3     Julia Manchester            5588        The Hill                  5578   \n",
       "4   The New York Times            5499  New York Times                  5499   \n",
       "5        Max Greenwood            5361        The Hill                  5361   \n",
       "6           Dave Quinn            5023          People                  5023   \n",
       "7       Jordain Carney            4846        The Hill                  4846   \n",
       "8     Michael D. Shear            4691  New York Times                  4691   \n",
       "9     Alexia Fernandez            4326          People                  4326   \n",
       "10     Stephanie Petit            4191          People                  4191   \n",
       "11     Karen Mizoguchi            4172          People                  4172   \n",
       "12        Maggie Astor            4155  New York Times                  4155   \n",
       "13     Nick Corasaniti            3789  New York Times                  3789   \n",
       "14         Sarah Perez            3749      TechCrunch                  3749   \n",
       "\n",
       "    main_outlet_share  as_mean_score  as_articles@center  as_articles@left  \\\n",
       "0             1.00000            0.0              7085.0               0.0   \n",
       "1             1.00000            0.0              5996.0               0.0   \n",
       "2             1.00000            0.0              5732.0               0.0   \n",
       "3             0.99821            0.0              5578.0               0.0   \n",
       "4             1.00000            NaN                 NaN               NaN   \n",
       "5             1.00000            0.0              5361.0               0.0   \n",
       "6             1.00000            NaN                 NaN               NaN   \n",
       "7             1.00000            0.0              4846.0               0.0   \n",
       "8             1.00000            NaN                 NaN               NaN   \n",
       "9             1.00000            NaN                 NaN               NaN   \n",
       "10            1.00000            NaN                 NaN               NaN   \n",
       "11            1.00000            NaN                 NaN               NaN   \n",
       "12            1.00000            NaN                 NaN               NaN   \n",
       "13            1.00000            NaN                 NaN               NaN   \n",
       "14            1.00000            0.0              3749.0               0.0   \n",
       "\n",
       "    as_articles@left-center  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       NaN  \n",
       "5                       0.0  \n",
       "6                       NaN  \n",
       "7                       0.0  \n",
       "8                       NaN  \n",
       "9                       NaN  \n",
       "10                      NaN  \n",
       "11                      NaN  \n",
       "12                      NaN  \n",
       "13                      NaN  \n",
       "14                      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 5.1) PER-AUTHOR ALLSIDES RATING (WEIGHTED)\n",
    "# =========================\n",
    "\n",
    "# ---- Inputs/paths\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "COUNTS_PATH = OUT_DIR / \"author_outlet_counts_humanlike.csv\"\n",
    "ALLSIDES_PATH = OUT_DIR / \"allsides_outlet_ratings.csv\"       # produced in Section 3\n",
    "\n",
    "# ---- Load data\n",
    "counts = pd.read_csv(COUNTS_PATH)  # columns: author, publication, domain, article_count\n",
    "allsides = pd.read_csv(ALLSIDES_PATH)  # expected: source_name, allsides_label, allsides_score5, type, ...\n",
    "\n",
    "# ---- Helpers\n",
    "def clean_name(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Canonicalize outlet names for joining:\n",
    "    - lowercase\n",
    "    - strip whitespace\n",
    "    - remove leading 'the '\n",
    "    - collapse multiple spaces and remove some punctuation\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"^the\\s+\", \"\", s)\n",
    "    s = re.sub(r\"[’'`]\", \"\", s)      # normalize apostrophes\n",
    "    s = re.sub(r\"\\s*&\\s*\", \" and \", s)\n",
    "    s = re.sub(r\"[-_]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# OPTIONAL: manual crosswalk for tricky names (extend freely)\n",
    "PUB_TO_ALLSIDES = {\n",
    "    # examples you can uncomment/extend:\n",
    "    # \"washington post\": \"washington post\",\n",
    "    # \"new york times\": \"new york times\",\n",
    "    # \"vice news\": \"vice\",\n",
    "    # \"buzzfeed news\": \"buzzfeed news\",\n",
    "    # \"the hill\": \"the hill\",\n",
    "    # add more as needed...\n",
    "}\n",
    "\n",
    "# ---- Prepare join keys\n",
    "counts['publication_for_join'] = counts['publication'].astype(str).str.strip().str.lower()\n",
    "counts['publication_for_join'] = counts['publication_for_join'].replace(PUB_TO_ALLSIDES)\n",
    "counts['pub_clean'] = counts['publication_for_join'].map(clean_name)\n",
    "\n",
    "allsides['source_name'] = allsides['source_name'].astype(str)\n",
    "allsides['source_clean'] = allsides['source_name'].map(clean_name)\n",
    "\n",
    "# Keep only News/Media rows if you didn't filter earlier\n",
    "if 'type' in allsides.columns:\n",
    "    mask_news = allsides['type'].str.contains(\"news|media\", case=False, na=False)\n",
    "    allsides = allsides[mask_news].copy()\n",
    "\n",
    "# ---- Merge by cleaned outlet name\n",
    "joined = counts.merge(\n",
    "    allsides[['source_clean', 'allsides_label', 'allsides_score5']],\n",
    "    left_on='pub_clean', right_on='source_clean', how='left'\n",
    ")\n",
    "\n",
    "# ---- Aggregate per author\n",
    "# Total articles and main outlet/share\n",
    "author_totals = (\n",
    "    joined.groupby('author', as_index=False)['article_count'].sum()\n",
    "          .rename(columns={'article_count': 'total_articles'})\n",
    ")\n",
    "\n",
    "main_outlet = (joined\n",
    "    .sort_values(['author', 'article_count'], ascending=[True, False])\n",
    "    .groupby('author', as_index=False).first()[['author', 'publication', 'article_count']]\n",
    "    .rename(columns={'publication': 'main_outlet', 'article_count': 'main_outlet_articles'})\n",
    ")\n",
    "\n",
    "author_core = author_totals.merge(main_outlet, on='author', how='left')\n",
    "author_core['main_outlet_share'] = author_core['main_outlet_articles'] / author_core['total_articles']\n",
    "\n",
    "# Weighted mean AllSides score per author\n",
    "def wavg(series, weights):\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    w = weights.loc[s.index]\n",
    "    return float(np.average(s, weights=w))\n",
    "\n",
    "as_mean = (\n",
    "    joined.groupby('author')['allsides_score5']\n",
    "          .apply(lambda s: wavg(s, joined.loc[s.index, 'article_count']))\n",
    "          .rename('as_mean_score')\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Optional: per-label article counts (pivot)\n",
    "as_label_counts = pd.pivot_table(\n",
    "    joined,\n",
    "    index='author',\n",
    "    columns='allsides_label',\n",
    "    values='article_count',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "# prefix columns\n",
    "as_label_counts.columns = [f\"as_articles@{c}\" for c in as_label_counts.columns]\n",
    "as_label_counts = as_label_counts.reset_index()\n",
    "\n",
    "# ---- Final per-author AllSides table\n",
    "per_author_allsides = (\n",
    "    author_core\n",
    "    .merge(as_mean, on='author', how='left')\n",
    "    .merge(as_label_counts, on='author', how='left')\n",
    "    .sort_values('total_articles', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---- Save\n",
    "out_path_as = OUT_DIR / \"per_author_allsides_weighted.csv\"\n",
    "per_author_allsides.to_csv(out_path_as, index=False)\n",
    "print(\"Saved:\", out_path_as)\n",
    "display(per_author_allsides.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b50ea1",
   "metadata": {},
   "source": [
    "### 5.2. MBFC\n",
    "MBFC data already has a domain column in the processed file, so we can join on ```domain``` directly.\n",
    "This computes:\n",
    "\n",
    "- ```mbfc_mean_score7``` (article-weighted mean on a [-3..+3] scale)\n",
    "- optional **factuality mean** (map MBFC factual_reporting to a numeric scale)\n",
    "- optional label counts per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1133a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/per_author_mbfc_weighted.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>main_outlet</th>\n",
       "      <th>main_outlet_articles</th>\n",
       "      <th>main_outlet_share</th>\n",
       "      <th>mbfc_mean_score7</th>\n",
       "      <th>mbfc_factuality_mean</th>\n",
       "      <th>mbfc_articles@left</th>\n",
       "      <th>mbfc_articles@left-center</th>\n",
       "      <th>mbfc_articles@neutral</th>\n",
       "      <th>mbfc_articles@right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bowden</td>\n",
       "      <td>7085</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>7085</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7085.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Savransky</td>\n",
       "      <td>5996</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5996</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5996.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brett Samuels</td>\n",
       "      <td>5732</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5732</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5732.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julia Manchester</td>\n",
       "      <td>5588</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5578</td>\n",
       "      <td>0.99821</td>\n",
       "      <td>-0.003579</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>5499</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>5499</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Max Greenwood</td>\n",
       "      <td>5361</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>5361</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5361.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave Quinn</td>\n",
       "      <td>5023</td>\n",
       "      <td>People</td>\n",
       "      <td>5023</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jordain Carney</td>\n",
       "      <td>4846</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>4846</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4846.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Michael D. Shear</td>\n",
       "      <td>4691</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>4691</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4691.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexia Fernandez</td>\n",
       "      <td>4326</td>\n",
       "      <td>People</td>\n",
       "      <td>4326</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4326.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stephanie Petit</td>\n",
       "      <td>4191</td>\n",
       "      <td>People</td>\n",
       "      <td>4191</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4191.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Karen Mizoguchi</td>\n",
       "      <td>4172</td>\n",
       "      <td>People</td>\n",
       "      <td>4172</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maggie Astor</td>\n",
       "      <td>4155</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>4155</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nick Corasaniti</td>\n",
       "      <td>3789</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>3789</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3789.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>3749</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>3749</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3749.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  total_articles     main_outlet  main_outlet_articles  \\\n",
       "0          John Bowden            7085        The Hill                  7085   \n",
       "1    Rebecca Savransky            5996        The Hill                  5996   \n",
       "2        Brett Samuels            5732        The Hill                  5732   \n",
       "3     Julia Manchester            5588        The Hill                  5578   \n",
       "4   The New York Times            5499  New York Times                  5499   \n",
       "5        Max Greenwood            5361        The Hill                  5361   \n",
       "6           Dave Quinn            5023          People                  5023   \n",
       "7       Jordain Carney            4846        The Hill                  4846   \n",
       "8     Michael D. Shear            4691  New York Times                  4691   \n",
       "9     Alexia Fernandez            4326          People                  4326   \n",
       "10     Stephanie Petit            4191          People                  4191   \n",
       "11     Karen Mizoguchi            4172          People                  4172   \n",
       "12        Maggie Astor            4155  New York Times                  4155   \n",
       "13     Nick Corasaniti            3789  New York Times                  3789   \n",
       "14         Sarah Perez            3749      TechCrunch                  3749   \n",
       "\n",
       "    main_outlet_share  mbfc_mean_score7  mbfc_factuality_mean  \\\n",
       "0             1.00000          0.000000                   3.0   \n",
       "1             1.00000          0.000000                   3.0   \n",
       "2             1.00000          0.000000                   3.0   \n",
       "3             0.99821         -0.003579                   3.0   \n",
       "4             1.00000         -1.000000                   5.0   \n",
       "5             1.00000          0.000000                   3.0   \n",
       "6             1.00000         -2.000000                   5.0   \n",
       "7             1.00000          0.000000                   3.0   \n",
       "8             1.00000         -1.000000                   5.0   \n",
       "9             1.00000         -2.000000                   5.0   \n",
       "10            1.00000         -2.000000                   5.0   \n",
       "11            1.00000         -2.000000                   5.0   \n",
       "12            1.00000         -1.000000                   5.0   \n",
       "13            1.00000         -1.000000                   5.0   \n",
       "14            1.00000         -1.000000                   5.0   \n",
       "\n",
       "    mbfc_articles@left  mbfc_articles@left-center  mbfc_articles@neutral  \\\n",
       "0                  0.0                        0.0                 7085.0   \n",
       "1                  0.0                        0.0                 5996.0   \n",
       "2                  0.0                        0.0                 5732.0   \n",
       "3                 10.0                        0.0                 5578.0   \n",
       "4                  0.0                     5499.0                    0.0   \n",
       "5                  0.0                        0.0                 5361.0   \n",
       "6               5023.0                        0.0                    0.0   \n",
       "7                  0.0                        0.0                 4846.0   \n",
       "8                  0.0                     4691.0                    0.0   \n",
       "9               4326.0                        0.0                    0.0   \n",
       "10              4191.0                        0.0                    0.0   \n",
       "11              4172.0                        0.0                    0.0   \n",
       "12                 0.0                     4155.0                    0.0   \n",
       "13                 0.0                     3789.0                    0.0   \n",
       "14                 0.0                     3749.0                    0.0   \n",
       "\n",
       "    mbfc_articles@right  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "5                   0.0  \n",
       "6                   0.0  \n",
       "7                   0.0  \n",
       "8                   0.0  \n",
       "9                   0.0  \n",
       "10                  0.0  \n",
       "11                  0.0  \n",
       "12                  0.0  \n",
       "13                  0.0  \n",
       "14                  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 5.2) PER-AUTHOR MBFC RATING (WEIGHTED BY DOMAIN)\n",
    "# =========================\n",
    "\n",
    "# ---- Inputs/paths\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "COUNTS_PATH = OUT_DIR / \"author_outlet_counts_humanlike.csv\"\n",
    "MBFC_PATH = OUT_DIR / \"mbfc_outlet_ratings.csv\"               # produced in Section 4\n",
    "\n",
    "# ---- Load data\n",
    "counts = pd.read_csv(COUNTS_PATH)  # columns: author, publication, domain, article_count\n",
    "mbfc = pd.read_csv(MBFC_PATH)      # expected: domain, mbfc_label, mbfc_score7, (optional) mbfc_factuality\n",
    "\n",
    "# ---- Merge by domain\n",
    "joined = counts.merge(\n",
    "    mbfc[['domain', 'mbfc_label', 'mbfc_score7'] + ([c for c in ['mbfc_factuality'] if c in mbfc.columns])],\n",
    "    on='domain', how='left'\n",
    ")\n",
    "\n",
    "# ---- Aggregate per author (reuse totals and main outlet/share)\n",
    "author_totals = (\n",
    "    joined.groupby('author', as_index=False)['article_count'].sum()\n",
    "          .rename(columns={'article_count': 'total_articles'})\n",
    ")\n",
    "main_outlet = (joined\n",
    "    .sort_values(['author', 'article_count'], ascending=[True, False])\n",
    "    .groupby('author', as_index=False).first()[['author', 'publication', 'article_count']]\n",
    "    .rename(columns={'publication': 'main_outlet', 'article_count': 'main_outlet_articles'})\n",
    ")\n",
    "author_core = author_totals.merge(main_outlet, on='author', how='left')\n",
    "author_core['main_outlet_share'] = author_core['main_outlet_articles'] / author_core['total_articles']\n",
    "\n",
    "# Weighted mean MBFC bias score per author\n",
    "def wavg(series, weights):\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    w = weights.loc[s.index]\n",
    "    return float(np.average(s, weights=w))\n",
    "\n",
    "mbfc_mean = (\n",
    "    joined.groupby('author')['mbfc_score7']\n",
    "          .apply(lambda s: wavg(s, joined.loc[s.index, 'article_count']))\n",
    "          .rename('mbfc_mean_score7')\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Optional: factuality → numeric mapping (tweak as needed to match your CSV)\n",
    "FACTUALITY_TO_NUM = {\n",
    "    # map whatever appears in your mbfc.csv\n",
    "    'very low': 1, 'low': 2, 'mixed': 3,\n",
    "    'mostly factual': 4, 'high': 5, 'very high': 6,\n",
    "    'na': np.nan, 'none': np.nan\n",
    "}\n",
    "if 'mbfc_factuality' in joined.columns:\n",
    "    j2 = joined.copy()\n",
    "    j2['mbfc_fact_num'] = j2['mbfc_factuality'].astype(str).str.lower().map(FACTUALITY_TO_NUM)\n",
    "    mbfc_fact_mean = (\n",
    "        j2.groupby('author')['mbfc_fact_num']\n",
    "          .apply(lambda s: wavg(s, j2.loc[s.index, 'article_count']))\n",
    "          .rename('mbfc_factuality_mean')\n",
    "          .reset_index()\n",
    "    )\n",
    "else:\n",
    "    mbfc_fact_mean = pd.DataFrame({'author': [], 'mbfc_factuality_mean': []})\n",
    "\n",
    "# Optional: per-label article counts (pivot)\n",
    "mbfc_label_counts = pd.pivot_table(\n",
    "    joined,\n",
    "    index='author',\n",
    "    columns='mbfc_label',\n",
    "    values='article_count',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "mbfc_label_counts.columns = [f\"mbfc_articles@{c}\" for c in mbfc_label_counts.columns]\n",
    "mbfc_label_counts = mbfc_label_counts.reset_index()\n",
    "\n",
    "# ---- Final per-author MBFC table\n",
    "per_author_mbfc = (\n",
    "    author_core\n",
    "    .merge(mbfc_mean, on='author', how='left')\n",
    "    .merge(mbfc_fact_mean, on='author', how='left')\n",
    "    .merge(mbfc_label_counts, on='author', how='left')\n",
    "    .sort_values('total_articles', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---- Save\n",
    "out_path_mb = OUT_DIR / \"per_author_mbfc_weighted.csv\"\n",
    "per_author_mbfc.to_csv(out_path_mb, index=False)\n",
    "print(\"Saved:\", out_path_mb)\n",
    "display(per_author_mbfc.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5958698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
