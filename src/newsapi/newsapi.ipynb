{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45323b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NEWSAPI_API_KEY = os.getenv(\"NEWSAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5e42d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSource:\n",
    "    def __init__(self, key: str, domain: str, name: str, mbfc_rating: str):\n",
    "        self.key = key\n",
    "        self.domain = domain\n",
    "        self.name = name\n",
    "        self.mbfc_rating = mbfc_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a34e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init all news sources\n",
    "\"\"\"\n",
    "- Right\n",
    "    - Fox News - [foxnews.com](http://foxnews.com)\n",
    "    - Breitbart - [breitbart.com](http://breitbart.com)\n",
    "    - Dailymail - [dailymail.co.uk](http://dailymail.co.uk) → UK\n",
    "    - The Sun - [thesun.co.uk](http://thesun.co.uk/) → UK\n",
    "- Right-Center\n",
    "    - Wall Street Journal - [wsj.com](http://wsj.com)\n",
    "    - New York Post - [nypost.com](http://nypost.com/)\n",
    "    - Forbes - [forbes.com](http://forbes.com)\n",
    "    - India Times - [indiatimes.com](http://indiatimes.com) → India\n",
    "    - News Week - [newsweek.com](http://newsweek.com/)\n",
    "- Neutral\n",
    "    - Reuters - [reuters.com](http://reuters.com)\n",
    "    - The Hill - [thehill.com](http://thehill.com)\n",
    "- Left-Center\n",
    "    - The New York Times - [nytimes.com](http://nytimes.com)\n",
    "    - The Washington Post - [washingtonpost.com](http://washingtonpost.com)\n",
    "    - USA Today - [usatoday.com](http://usatoday.com)\n",
    "    - Buzz Feed - [buzzfeed.com](http://buzzfeed.com)\n",
    "    - CBS News - [cbsnews.com](http://cbsnews.com)\n",
    "    - SF Gate - [sfgate.com](http://sfgate.com/)\n",
    "    - Bloomberg - [bloomberg.com](http://bloomberg.com)\n",
    "- Left\n",
    "    - CNN - [cnn.com](http://cnn.com)\n",
    "    - People - [people.com](http://people.com)\n",
    "\"\"\"\n",
    "news_sources = [\n",
    "    # Right\n",
    "    NewsSource(\"foxnews\", \"foxnews.com\", \"Fox News\", \"Right\"),\n",
    "    NewsSource(\"breitbart\", \"breitbart.com\", \"Breitbart\", \"Right\"),\n",
    "    NewsSource(\"dailymail\", \"dailymail.co.uk\", \"Daily Mail\", \"Right\"),\n",
    "    NewsSource(\"thesun\", \"thesun.co.uk\", \"The Sun\", \"Right\"),\n",
    "    # Right-Center\n",
    "    NewsSource(\"wsj\", \"wsj.com\", \"Wall Street Journal\", \"Right-Center\"),\n",
    "    NewsSource(\"nypost\", \"nypost.com\", \"New York Post\", \"Right-Center\"),\n",
    "    NewsSource(\"forbes\", \"forbes.com\", \"Forbes\", \"Right-Center\"),\n",
    "    NewsSource(\"indiatimes\", \"indiatimes.com\", \"India Times\", \"Right-Center\"),\n",
    "    NewsSource(\"newsweek\", \"newsweek.com\", \"Newsweek\", \"Right-Center\"),\n",
    "    # Neutral\n",
    "    NewsSource(\"reuters\", \"reuters.com\", \"Reuters\", \"Neutral\"),\n",
    "    NewsSource(\"thehill\", \"thehill.com\", \"The Hill\", \"Neutral\"),\n",
    "    # Left-Center\n",
    "    NewsSource(\"nytimes\", \"nytimes.com\", \"The New York Times\", \"Left-Center\"),\n",
    "    NewsSource(\"washingtonpost\", \"washingtonpost.com\", \"The Washington Post\", \"Left-Center\"),\n",
    "    NewsSource(\"usatoday\", \"usatoday.com\", \"USA Today\", \"Left-Center\"),\n",
    "    NewsSource(\"buzzfeed\", \"buzzfeed.com\", \"BuzzFeed\", \"Left-Center\"),\n",
    "    NewsSource(\"cbsnews\", \"cbsnews.com\", \"CBS News\", \"Left-Center\"),\n",
    "    NewsSource(\"sfgate\", \"sfgate.com\", \"SF Gate\", \"Left-Center\"),\n",
    "    NewsSource(\"bloomberg\", \"bloomberg.com\", \"Bloomberg\", \"Left-Center\"),\n",
    "    # Left\n",
    "    NewsSource(\"cnn\", \"cnn.com\", \"CNN\", \"Left\"),\n",
    "    NewsSource(\"people\", \"people.com\", \"People\", \"Left\"),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ce3f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../../outputs/newsapi/\"\n",
    "ARTICLES_PATH = BASE_PATH + \"articles/politics\"\n",
    "AUTHORS_PATH = BASE_PATH + \"authors/politics\"\n",
    "FIGURES_PATH = BASE_PATH + \"figures/politics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd7612",
   "metadata": {},
   "source": [
    "## 1. Fetch The Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a651e2a",
   "metadata": {},
   "source": [
    "### 1.1. Fetch Articles Using News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import EventRegistry, QueryArticlesIter\n",
    "er = EventRegistry(apiKey=NEWSAPI_API_KEY)\n",
    "max_articles = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in news_sources:\n",
    "    q = QueryArticlesIter(\n",
    "        sourceUri = source.domain,\n",
    "        lang = \"eng\",\n",
    "        dateStart = \"2025-01-01\",\n",
    "    )\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for art in q.execQuery(er, sortBy = \"socialScore\", maxItems = 10000):\n",
    "        articles.append(art)\n",
    "    \n",
    "    # store the articles in a json file\n",
    "    with open(f\"{ARTICLES_PATH}{source.key}_articles.json\", \"w\") as f:\n",
    "        json.dump(articles, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e1a0b",
   "metadata": {},
   "source": [
    "### 1.2. Fetch Articles Using Topic Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326ee35",
   "metadata": {},
   "source": [
    "* Here, we retrieve articles using the [Topic Page](https://eventregistry.org/monitoring?uri=e13d6751-0c0c-4970-8f60-09a65a13b5b0) we have created in [News API](newsapi.ai).\n",
    "* The topic page includes the topic `news:politics` and is configured to retrieve articles within last 30 days from sources above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b9dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import EventRegistry, TopicPage\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "er = EventRegistry(apiKey=NEWSAPI_API_KEY)\n",
    "\n",
    "POLITICS_TOPIC_PAGE_URI = \"e13d6751-0c0c-4970-8f60-09a65a13b5b0\" # Public topic page for topic -> news:politics and for news sources above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6525480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving articles: 100%|██████████| 213/213 [04:55<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "t = TopicPage(er)\n",
    "# load the topic page with the given uri\n",
    "t.loadTopicPageFromER(POLITICS_TOPIC_PAGE_URI)\n",
    "\n",
    "res = t.getArticles(page=1) # first get total pages.\n",
    "total_pages = res[\"articles\"][\"pages\"]\n",
    "\n",
    "articles = []\n",
    "# retrieve articles page by page.\n",
    "for page_num in tqdm(range(1, total_pages + 1), desc=\"Retrieving articles\"):\n",
    "    res = t.getArticles(page=page_num) # call the api to get the articles\n",
    "    for art in res.get(\"articles\", {}).get(\"results\", []): # append the articles to the list\n",
    "        articles.append(art)\n",
    "    \n",
    "# Save articles to a json file\n",
    "os.makedirs(ARTICLES_PATH, exist_ok=True)\n",
    "with open(f\"{ARTICLES_PATH}/politics_articles.json\", \"w\") as f:\n",
    "    json.dump(articles, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b914848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles retrieved: 21295\n"
     ]
    }
   ],
   "source": [
    "# Show number of articles retrieved\n",
    "print(\"Number of articles retrieved:\", len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc7d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save articles to a json file\n",
    "os.makedirs(ARTICLES_PATH, exist_ok=True)\n",
    "with open(f\"{ARTICLES_PATH}/politics_articles.json\", \"w\") as f:\n",
    "    json.dump(articles, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbfdd8",
   "metadata": {},
   "source": [
    "## 2. Analyze Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ece092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9be0f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing foxnews... 1/20\n",
      "Processing breitbart... 2/20\n",
      "Processing dailymail... 3/20\n",
      "Processing thesun... 4/20\n",
      "Processing wsj... 5/20\n",
      "Processing nypost... 6/20\n",
      "Processing forbes... 7/20\n",
      "Processing indiatimes... 8/20\n",
      "Processing newsweek... 9/20\n",
      "Processing reuters... 10/20\n",
      "Processing thehill... 11/20\n",
      "Processing nytimes... 12/20\n",
      "Processing washingtonpost... 13/20\n",
      "Processing usatoday... 14/20\n",
      "Processing buzzfeed... 15/20\n",
      "Processing cbsnews... 16/20\n",
      "Processing sfgate... 17/20\n",
      "Processing bloomberg... 18/20\n",
      "Processing cnn... 19/20\n",
      "Processing people... 20/20\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for i, source in enumerate(news_sources):\n",
    "    print(f\"Processing {source.key}... {i+1}/{len(news_sources)}\")\n",
    "    # Load the articles\n",
    "    with open(f\"{ARTICLES_PATH}{source.key}_articles.json\", \"r\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    authors2count = {}\n",
    "\n",
    "    for article in articles:\n",
    "        authors = article.get(\"authors\", [])\n",
    "        for author in authors:\n",
    "            author_name = author.get(\"name\", \"\")\n",
    "            if author_name in authors2count:\n",
    "                authors2count[author_name][\"count\"] += 1\n",
    "            else:\n",
    "                authors2count[author_name] = {\n",
    "                    \"name\": author_name,\n",
    "                    \"count\": 1,\n",
    "                    \"source\": source.key,\n",
    "                    \"mbfc_rating\": source.mbfc_rating,\n",
    "                    \"percentage\": 0\n",
    "                }\n",
    "\n",
    "    # Calculate the percentage\n",
    "    total_count = sum(author[\"count\"] for author in authors2count.values())\n",
    "    for author in authors2count:\n",
    "        authors2count[author][\"percentage\"] = authors2count[author][\"count\"] / total_count\n",
    "\n",
    "    # Sort the authors by count\n",
    "    authors2count = sorted(authors2count.values(), key=lambda x: x[\"count\"], reverse=True)\n",
    "\n",
    "    # Write the results to a csv file\n",
    "    authors_df = pd.DataFrame(authors2count)\n",
    "    authors_df.to_csv(f\"{AUTHORS_PATH}{source.key}_authors.csv\", index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "351733fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for forbes[forbes.com]\n",
      "No articles found for usatoday[usatoday.com]\n",
      "No articles found for buzzfeed[buzzfeed.com]\n",
      "No articles found for cnn[cnn.com]\n"
     ]
    }
   ],
   "source": [
    "# Merge all the authors csv files (sorted by percentage)\n",
    "merged_authors_df = pd.DataFrame()\n",
    "for source in news_sources:\n",
    "    try: \n",
    "        authors_df = pd.read_csv(f\"{AUTHORS_PATH}{source.key}_authors.csv\")\n",
    "        merged_authors_df = pd.concat([merged_authors_df, authors_df])\n",
    "    except:\n",
    "        print(f\"No articles found for {source.key}[{source.domain}]\")\n",
    "\n",
    "merged_authors_df = merged_authors_df.sort_values(by=\"percentage\", ascending=False)\n",
    "merged_authors_df.to_csv(f\"{AUTHORS_PATH}all_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f16977",
   "metadata": {},
   "source": [
    "## 3. Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d570b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing foxnews... 1/20\n",
      "  ⚠️ Missing file: foxnews_articles.json — skipping.\n",
      "Processing breitbart... 2/20\n",
      "  ⚠️ Missing file: breitbart_articles.json — skipping.\n",
      "Processing dailymail... 3/20\n",
      "  ⚠️ Missing file: dailymail_articles.json — skipping.\n",
      "Processing thesun... 4/20\n",
      "  ⚠️ Missing file: thesun_articles.json — skipping.\n",
      "Processing wsj... 5/20\n",
      "  ⚠️ Missing file: wsj_articles.json — skipping.\n",
      "Processing nypost... 6/20\n",
      "  ⚠️ Missing file: nypost_articles.json — skipping.\n",
      "Processing forbes... 7/20\n",
      "  ⚠️ Missing file: forbes_articles.json — skipping.\n",
      "Processing indiatimes... 8/20\n",
      "  ⚠️ Missing file: indiatimes_articles.json — skipping.\n",
      "Processing newsweek... 9/20\n",
      "  ⚠️ Missing file: newsweek_articles.json — skipping.\n",
      "Processing reuters... 10/20\n",
      "  ⚠️ Missing file: reuters_articles.json — skipping.\n",
      "Processing thehill... 11/20\n",
      "  ⚠️ Missing file: thehill_articles.json — skipping.\n",
      "Processing nytimes... 12/20\n",
      "  ⚠️ Missing file: nytimes_articles.json — skipping.\n",
      "Processing washingtonpost... 13/20\n",
      "  ⚠️ Missing file: washingtonpost_articles.json — skipping.\n",
      "Processing usatoday... 14/20\n",
      "  ⚠️ Missing file: usatoday_articles.json — skipping.\n",
      "Processing buzzfeed... 15/20\n",
      "  ⚠️ Missing file: buzzfeed_articles.json — skipping.\n",
      "Processing cbsnews... 16/20\n",
      "  ⚠️ Missing file: cbsnews_articles.json — skipping.\n",
      "Processing sfgate... 17/20\n",
      "  ⚠️ Missing file: sfgate_articles.json — skipping.\n",
      "Processing bloomberg... 18/20\n",
      "  ⚠️ Missing file: bloomberg_articles.json — skipping.\n",
      "Processing cnn... 19/20\n",
      "  ⚠️ Missing file: cnn_articles.json — skipping.\n",
      "Processing people... 20/20\n",
      "  ⚠️ Missing file: people_articles.json — skipping.\n",
      "✅ Done. Saved 20 per-source panels + 1 cumulative panel → /Users/baturalpkabadayi/Dev/Research/JournalistLLM/outputs/newsapi/figures/politics\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# News distributions panels\n",
    "# =========================\n",
    "# Requirements: pandas, matplotlib, numpy\n",
    "\n",
    "import os, json, re\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FIG_DIR = FIGURES_PATH             # where to save the PNGs\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Toggle if you want a log-scale x-axis for the author histogram (useful when long-tailed)\n",
    "AUTHOR_HIST_LOGX = False\n",
    "\n",
    "# Authors to ignore (case-insensitive). After cleaning, if an article's authors are all ignored, it counts as \"no author\".\n",
    "IGNORE_AUTHORS = {\n",
    "    \"reuters\",\n",
    "    \"pa media\",\n",
    "    \"editorial board\",\n",
    "    \"associated press\",\n",
    "    \"field level media\",\n",
    "    \"more\",\n",
    "\n",
    "}\n",
    "# Regex patterns to ignore (case-insensitive), applied to the cleaned name.\n",
    "IGNORE_AUTHOR_PATTERNS = [\n",
    "    re.compile(r\"\\b(desk|newsroom)\\b\", re.I),\n",
    "]\n",
    "# If True, drop authors where authors[].isAgency == True (when present in JSON).\n",
    "IGNORE_AGENCY_FLAG = True\n",
    "\n",
    "RATING_ORDER = [\"Right\", \"Right-Center\", \"Neutral\", \"Left-Center\", \"Left\"]\n",
    "\n",
    "# ---------- AUTHOR EXTRACTION ----------\n",
    "_SPLIT = re.compile(r'\\s*(?:,| and | & |;|\\|)\\s*', re.IGNORECASE)\n",
    "\n",
    "def _split_tokens(s: str) -> List[str]:\n",
    "    return [t for t in _SPLIT.split(s) if t and t.strip()]\n",
    "\n",
    "def _clean_name(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # Remove \"By ...\" prefix\n",
    "    s = re.sub(r'^\\s*by\\s+', '', s, flags=re.IGNORECASE)\n",
    "    low = s.lower()\n",
    "    # Drop obvious junk\n",
    "    if \"http://\" in low or \"https://\" in low or \".com\" in low or \".co.\" in low:\n",
    "        return \"\"\n",
    "    # Convert emails to a readable token (left side)\n",
    "    if \"@\" in s:\n",
    "        s = s.split(\"@\")[0].replace(\".\", \" \").strip()\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def _is_ignored_author(name: str, author_obj: Dict[str, Any] | None = None) -> bool:\n",
    "    if not name:\n",
    "        return True\n",
    "    if author_obj and IGNORE_AGENCY_FLAG:\n",
    "        try:\n",
    "            if bool(author_obj.get(\"isAgency\")):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    low = name.lower()\n",
    "    if low in IGNORE_AUTHORS:\n",
    "        return True\n",
    "    for pat in IGNORE_AUTHOR_PATTERNS:\n",
    "        if pat.search(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def normalize_authors_from_record(rec: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preferred: rec['authors'] = [{name, isAgency, ...}, ...]\n",
    "    Fallbacks: rec['author'] (str or list), rec['byline'] (str)\n",
    "    Returns a de-duplicated list of cleaned author names after applying ignore rules.\n",
    "    \"\"\"\n",
    "    names: List[str] = []\n",
    "\n",
    "    # Primary: authors[].name\n",
    "    if isinstance(rec.get('authors'), list):\n",
    "        for a in rec['authors']:\n",
    "            if isinstance(a, dict):\n",
    "                nm = _clean_name(a.get('name', ''))\n",
    "                if nm and not _is_ignored_author(nm, a):\n",
    "                    names.append(nm)\n",
    "            else:\n",
    "                nm = _clean_name(str(a))\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "\n",
    "    # Fallback: author (str or list)\n",
    "    if not names and ('author' in rec):\n",
    "        auth = rec['author']\n",
    "        if isinstance(auth, list):\n",
    "            for a in auth:\n",
    "                nm = _clean_name(str(a))\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "        else:\n",
    "            for p in _split_tokens(str(auth)):\n",
    "                nm = _clean_name(p)\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "\n",
    "    # Fallback: byline (str)\n",
    "    if not names and isinstance(rec.get('byline'), str):\n",
    "        for p in _split_tokens(rec['byline']):\n",
    "            nm = _clean_name(p)\n",
    "            if nm and not _is_ignored_author(nm, None):\n",
    "                names.append(nm)\n",
    "\n",
    "    # de-dup, preserve order\n",
    "    seen, out = set(), []\n",
    "    for n in names:\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); out.append(n)\n",
    "    return out\n",
    "\n",
    "def explode_authors_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['authors_norm'] = df.apply(normalize_authors_from_record, axis=1)\n",
    "    exploded = df.explode('authors_norm', ignore_index=True)\n",
    "    exploded['has_author'] = exploded['authors_norm'].notna()\n",
    "    return exploded\n",
    "\n",
    "# ---------- LOAD & METRICS ----------\n",
    "def load_source_df(source_key: str) -> pd.DataFrame:\n",
    "    fp = os.path.join(ARTICLES_PATH, f\"{source_key}_articles.json\")\n",
    "    with open(fp, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # supports list of records OR {\"articles\":[...]}\n",
    "    if isinstance(data, dict) and 'articles' in data:\n",
    "        data = data['articles']\n",
    "    df = pd.DataFrame(data)\n",
    "    # ensure a sentiment column exists and is numeric in [-1, 1]\n",
    "    if 'sentiment' not in df.columns:\n",
    "        df['sentiment'] = np.nan\n",
    "    df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce').clip(-1, 1)\n",
    "    return df\n",
    "\n",
    "def per_author_article_counts(exploded_author_series: pd.Series) -> pd.Series:\n",
    "    return exploded_author_series.value_counts()\n",
    "\n",
    "def author_hist_bins(counts_per_author: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sensible bins for right-skewed author counts.\n",
    "    - If max <= 40 → integer-centered bins: [0.5, 1.5, ..., max+0.5]\n",
    "    - Else → 30 linear bins [1, max]\n",
    "    \"\"\"\n",
    "    if counts_per_author.empty:\n",
    "        return np.array([])\n",
    "    max_k = int(counts_per_author.max())\n",
    "    if max_k <= 40:\n",
    "        return np.arange(0.5, max_k + 1.5, 1.0)\n",
    "    return np.linspace(1, max_k, 30)\n",
    "\n",
    "def author_count_stats(counts_per_author: pd.Series) -> Tuple[float, float]:\n",
    "    if counts_per_author.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    mu = float(counts_per_author.mean())\n",
    "    sigma = float(counts_per_author.std(ddof=1)) if len(counts_per_author) > 1 else 0.0\n",
    "    return mu, sigma\n",
    "\n",
    "def sentiment_stats(sentiments: pd.Series) -> Dict[str, float]:\n",
    "    s = sentiments.dropna().astype(float).clip(-1, 1)\n",
    "    if s.empty:\n",
    "        return dict(mean=np.nan, std=np.nan, pos=0.0, neu=1.0, neg=0.0)\n",
    "    neg = (s < -0.2).mean()\n",
    "    neu = ((s >= -0.2) & (s <= 0.2)).mean()\n",
    "    pos = (s > 0.2).mean()\n",
    "    return dict(\n",
    "        mean=float(s.mean()),\n",
    "        std=float(s.std(ddof=1)) if len(s) > 1 else 0.0,\n",
    "        pos=float(pos), neu=float(neu), neg=float(neg),\n",
    "    )\n",
    "\n",
    "def _label_hbar_counts(ax):\n",
    "    for p in ax.patches:\n",
    "        w = p.get_width()\n",
    "        if w > 0:\n",
    "            ax.annotate(f'{int(w)}', (w, p.get_y()+p.get_height()/2),\n",
    "                        ha='left', va='center', fontsize=9, xytext=(3,0), textcoords='offset points')\n",
    "\n",
    "# ---------- PANELS ----------\n",
    "def _plot_log_author_hist(ax, counts_per_author: pd.Series, title_prefix: str):\n",
    "    \"\"\"\n",
    "    Bottom-right tile: histogram of log10(articles per author).\n",
    "    Shows μ and σ in log space and highlights modal bin.\n",
    "    \"\"\"\n",
    "    if counts_per_author.empty:\n",
    "        ax.text(0.5, 0.5, \"No author data\", ha='center', va='center')\n",
    "        ax.set_title(f\"{title_prefix} Log Author Count Distribution\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        return\n",
    "\n",
    "    vals = counts_per_author.values.astype(float)\n",
    "    vals = vals[vals > 0]  # safety\n",
    "    log_vals = np.log10(vals)\n",
    "\n",
    "    # bins: auto works well on log data; fall back to 20 if needed\n",
    "    try:\n",
    "        bins = np.histogram_bin_edges(log_vals, bins='auto')\n",
    "    except Exception:\n",
    "        bins = np.linspace(log_vals.min(), log_vals.max(), 20)\n",
    "\n",
    "    n, b, patches = ax.hist(log_vals, bins=bins, edgecolor='black')\n",
    "    # highlight modal bin\n",
    "    if len(n) > 0:\n",
    "        modal_idx = int(np.argmax(n))\n",
    "        if 0 <= modal_idx < len(patches):\n",
    "            patches[modal_idx].set_facecolor('tab:orange')\n",
    "\n",
    "    mu_log = float(log_vals.mean()) if log_vals.size else np.nan\n",
    "    sd_log = float(log_vals.std(ddof=1)) if log_vals.size > 1 else 0.0\n",
    "    if not np.isnan(mu_log):\n",
    "        ax.axvline(mu_log, linestyle='-', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel(\"log10(articles per author)\")\n",
    "    ax.set_ylabel(\"# of authors\")\n",
    "    ax.set_title(f\"{title_prefix} Log Author Count Distribution\")\n",
    "\n",
    "def make_panel_for_source(source: NewsSource, df: pd.DataFrame, save_dir: str):\n",
    "    # Standardize columns for safety\n",
    "    df = df.copy()\n",
    "    if 'sentiment' not in df.columns:\n",
    "        df['sentiment'] = np.nan\n",
    "    if not any(col in df.columns for col in ('authors','author','byline')):\n",
    "        df['authors'] = [[] for _ in range(len(df))]\n",
    "\n",
    "    total_articles = int(len(df))\n",
    "\n",
    "    # Authors (with ignore filtering)\n",
    "    author_lists = df.apply(normalize_authors_from_record, axis=1)\n",
    "    no_author_count = int((author_lists.apply(len) == 0).sum())\n",
    "\n",
    "    exploded = df.copy()\n",
    "    exploded['authors_norm'] = author_lists\n",
    "    exploded = exploded.explode('authors_norm', ignore_index=True)\n",
    "    exploded['has_author'] = exploded['authors_norm'].notna()\n",
    "\n",
    "    top_authors = (exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "                   .value_counts()\n",
    "                   .head(10))\n",
    "    top10_total = int(top_authors.sum()) if not top_authors.empty else 0\n",
    "\n",
    "    counts_per_author = per_author_article_counts(\n",
    "        exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "    )\n",
    "    mu_auth, sigma_auth = author_count_stats(counts_per_author)\n",
    "    bins_auth = author_hist_bins(counts_per_author)\n",
    "\n",
    "    # Sentiment (JSON only)\n",
    "    s_stats = sentiment_stats(df['sentiment'])\n",
    "    avg_sent, std_sent = s_stats['mean'], s_stats['std']\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(14, 10), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1,1])\n",
    "    ax11 = fig.add_subplot(gs[0,0])   # author count histogram (linear)\n",
    "    ax12 = fig.add_subplot(gs[0,1])   # top 10 authors\n",
    "    ax21 = fig.add_subplot(gs[1,0])   # sentiment histogram\n",
    "    ax22 = fig.add_subplot(gs[1,1])   # log author count histogram (NEW)\n",
    "\n",
    "    # (1,1) Author count histogram (linear x)\n",
    "    if not counts_per_author.empty and bins_auth.size > 0:\n",
    "        n, bins, patches = ax11.hist(counts_per_author.values, bins=bins_auth, edgecolor='black')\n",
    "        ax11.set_xlabel(\"Articles per author\")\n",
    "        ax11.set_ylabel(\"# of authors\")\n",
    "        ax11.set_title(f\"[{source.key}] Author→Article Count Distribution\")\n",
    "        # highlight modal bin\n",
    "        if len(n) > 0:\n",
    "            modal_idx = int(np.argmax(n))\n",
    "            if 0 <= modal_idx < len(patches):\n",
    "                patches[modal_idx].set_facecolor('tab:orange')\n",
    "        # mean line\n",
    "        if not np.isnan(mu_auth):\n",
    "            ax11.axvline(mu_auth, linestyle='-', linewidth=2)\n",
    "        if AUTHOR_HIST_LOGX:\n",
    "            ax11.set_xscale('log')\n",
    "        # annotation\n",
    "        ax11.text(0.98, 0.95,\n",
    "                  f\"Total articles: {total_articles}\\n\"\n",
    "                  f\"No-author articles: {no_author_count}\\n\"\n",
    "                  f\"μ (per-author count): {mu_auth:.2f}\\nσ: {sigma_auth:.2f}\",\n",
    "                  ha='right', va='top', transform=ax11.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax11.text(0.5, 0.5, \"No author data\", ha='center', va='center')\n",
    "        ax11.set_title(f\"[{source.key}] Author→Article Count Distribution\")\n",
    "        ax11.set_xticks([]); ax11.set_yticks([])\n",
    "\n",
    "    # (1,2) Top 10 authors\n",
    "    if not top_authors.empty:\n",
    "        top_authors.iloc[::-1].plot(kind='barh', ax=ax12)\n",
    "        ax12.set_xlabel(\"Article count\")\n",
    "        ax12.set_ylabel(\"Author\")\n",
    "        ax12.set_title(f\"[{source.key}] Top 10 Authors (total={top10_total})\")\n",
    "        # highlight the top bar (rightmost)\n",
    "        if len(ax12.patches) > 0:\n",
    "            ax12.patches[-1].set_facecolor('tab:red')\n",
    "        _label_hbar_counts(ax12)\n",
    "    else:\n",
    "        ax12.text(0.5, 0.5, \"No authors found\", ha='center', va='center')\n",
    "        ax12.set_title(f\"[{source.key}] Top 10 Authors (total=0)\")\n",
    "        ax12.set_xticks([]); ax12.set_yticks([])\n",
    "\n",
    "    # (2,1) Sentiment histogram\n",
    "    s = df['sentiment'].dropna().astype(float).clip(-1,1)\n",
    "    if not s.empty:\n",
    "        n_s, bins_s, patches_s = ax21.hist(s, bins=21, edgecolor='black')\n",
    "        ax21.set_xlim(-1, 1)\n",
    "        ax21.set_xlabel(\"Sentiment (JSON, [-1,1])\")\n",
    "        ax21.set_ylabel(\"# of articles\")\n",
    "        ax21.set_title(f\"[{source.key}] Sentiment Distribution\")\n",
    "        ax21.axvline(0, linestyle='--', linewidth=1)\n",
    "        if not np.isnan(avg_sent):\n",
    "            ax21.axvline(avg_sent, linestyle='-', linewidth=2)\n",
    "        ax21.text(0.98, 0.95,\n",
    "                  (f\"μ={avg_sent:.3f}\\nσ={std_sent:.3f}\\n\"\n",
    "                   f\"pos>{0.2}: {s_stats['pos']*100:.1f}%\\n\"\n",
    "                   f\"-0.2≤neu≤0.2: {s_stats['neu']*100:.1f}%\\n\"\n",
    "                   f\"neg<-0.2: {s_stats['neg']*100:.1f}%\")\n",
    "                  if not np.isnan(avg_sent) else \"μ=NaN\\nσ=NaN\",\n",
    "                  ha='right', va='top', transform=ax21.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax21.text(0.5, 0.5, \"No sentiment data in JSON\", ha='center', va='center')\n",
    "        ax21.set_title(f\"[{source.key}] Sentiment Distribution\")\n",
    "        ax21.set_xticks([]); ax21.set_yticks([])\n",
    "\n",
    "    # (2,2) NEW: Log author count histogram\n",
    "    _plot_log_author_hist(ax22, counts_per_author, f\"[{source.key}]\")\n",
    "\n",
    "    fig.suptitle(f\"{source.key}\", fontsize=14)  # (kept exactly as you asked)\n",
    "    out_path = os.path.join(save_dir, f\"{source.key}_panel.png\")\n",
    "    fig.savefig(out_path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "def make_panel_cumulative(all_records: List[Tuple[NewsSource, pd.DataFrame]], save_dir: str):\n",
    "    frames = []\n",
    "    for src, df in all_records:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        tmp = df.copy()\n",
    "        tmp['__rating__'] = src.mbfc_rating\n",
    "        tmp['__source__'] = src.key\n",
    "        # ensure sentiment column exists\n",
    "        if 'sentiment' not in tmp.columns:\n",
    "            tmp['sentiment'] = np.nan\n",
    "        frames.append(tmp)\n",
    "\n",
    "    if not frames:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.text(0.5, 0.5, \"No data across sources\", ha='center', va='center')\n",
    "        fig.savefig(os.path.join(save_dir, \"all_sources_panel.png\"), dpi=160)\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    big = pd.concat(frames, ignore_index=True)\n",
    "    if 'sentiment' not in big.columns:\n",
    "        big['sentiment'] = np.nan\n",
    "\n",
    "    # authors (WITH ignore filtering)\n",
    "    author_lists = big.apply(normalize_authors_from_record, axis=1)\n",
    "    exploded = big.copy()\n",
    "    exploded['authors_norm'] = author_lists\n",
    "    exploded = exploded.explode('authors_norm', ignore_index=True)\n",
    "    exploded['has_author'] = exploded['authors_norm'].notna()\n",
    "\n",
    "    top_authors = (exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "                   .value_counts()\n",
    "                   .head(10))\n",
    "    top10_total = int(top_authors.sum()) if not top_authors.empty else 0\n",
    "\n",
    "    counts_per_author = per_author_article_counts(\n",
    "        exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "    )\n",
    "    mu_auth, sigma_auth = author_count_stats(counts_per_author)\n",
    "    bins_auth = author_hist_bins(counts_per_author)\n",
    "\n",
    "    # sentiment\n",
    "    big['sentiment'] = pd.to_numeric(big['sentiment'], errors='coerce').clip(-1, 1)\n",
    "    s_stats = sentiment_stats(big['sentiment'])\n",
    "    avg_sent, std_sent = s_stats['mean'], s_stats['std']\n",
    "\n",
    "    # plotting\n",
    "    fig = plt.figure(figsize=(14, 10), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1,1])\n",
    "    ax11 = fig.add_subplot(gs[0,0])\n",
    "    ax12 = fig.add_subplot(gs[0,1])\n",
    "    ax21 = fig.add_subplot(gs[1,0])\n",
    "    ax22 = fig.add_subplot(gs[1,1])\n",
    "\n",
    "    # (1,1) cumulative author histogram (linear)\n",
    "    if not counts_per_author.empty and bins_auth.size > 0:\n",
    "        n, bins, patches = ax11.hist(counts_per_author.values, bins=bins_auth, edgecolor='black')\n",
    "        ax11.set_xlabel(\"Articles per author\")\n",
    "        ax11.set_ylabel(\"# of authors\")\n",
    "        ax11.set_title(\"[All] Author→Article Count Distribution\")\n",
    "        if len(n) > 0:\n",
    "            modal_idx = int(np.argmax(n))\n",
    "            if 0 <= modal_idx < len(patches):\n",
    "                patches[modal_idx].set_facecolor('tab:orange')\n",
    "        if not np.isnan(mu_auth):\n",
    "            ax11.axvline(mu_auth, linestyle='-', linewidth=2)\n",
    "        if AUTHOR_HIST_LOGX:\n",
    "            ax11.set_xscale('log')\n",
    "        ax11.text(0.98, 0.95,\n",
    "                  f\"μ (per-author count): {mu_auth:.2f}\\nσ: {sigma_auth:.2f}\",\n",
    "                  ha='right', va='top', transform=ax11.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax11.text(0.5, 0.5, \"No author data\", ha='center', va='center')\n",
    "        ax11.set_title(\"[All] Author→Article Count Distribution\")\n",
    "        ax11.set_xticks([]); ax11.set_yticks([])\n",
    "\n",
    "    # (1,2) cumulative top 10 authors\n",
    "    if not top_authors.empty:\n",
    "        top_authors.iloc[::-1].plot(kind='barh', ax=ax12)\n",
    "        ax12.set_xlabel(\"Article count\")\n",
    "        ax12.set_ylabel(\"Author\")\n",
    "        ax12.set_title(f\"[All] Top 10 Authors (total={top10_total})\")\n",
    "        if len(ax12.patches) > 0:\n",
    "            ax12.patches[-1].set_facecolor('tab:red')\n",
    "        _label_hbar_counts(ax12)\n",
    "    else:\n",
    "        ax12.text(0.5, 0.5, \"No authors found\", ha='center', va='center')\n",
    "        ax12.set_title(f\"[All] Top 10 Authors (total=0)\")\n",
    "        ax12.set_xticks([]); ax12.set_yticks([])\n",
    "\n",
    "    # (2,1) cumulative sentiment histogram\n",
    "    s = big['sentiment'].dropna().astype(float).clip(-1,1)\n",
    "    if not s.empty:\n",
    "        n_s, bins_s, patches_s = ax21.hist(s, bins=21, edgecolor='black')\n",
    "        ax21.set_xlim(-1, 1)\n",
    "        ax21.set_xlabel(\"Sentiment [-1,1]\")\n",
    "        ax21.set_ylabel(\"# of articles\")\n",
    "        ax21.set_title(\"[All] Sentiment Distribution\")\n",
    "        ax21.axvline(0, linestyle='--', linewidth=1)\n",
    "        if not np.isnan(avg_sent):\n",
    "            ax21.axvline(avg_sent, linestyle='-', linewidth=2)\n",
    "        ax21.text(0.98, 0.95,\n",
    "                  (f\"μ={avg_sent:.3f}\\nσ={std_sent:.3f}\"),\n",
    "                  ha='right', va='top', transform=ax21.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax21.text(0.5, 0.5, \"No sentiment data in JSON\", ha='center', va='center')\n",
    "        ax21.set_title(\"[All] Sentiment Distribution\")\n",
    "        ax21.set_xticks([]); ax21.set_yticks([])\n",
    "\n",
    "    # (2,2) NEW: cumulative log author count histogram\n",
    "    _plot_log_author_hist(ax22, counts_per_author, \"[All]\")\n",
    "\n",
    "    fig.suptitle(\"All Sources\", fontsize=14)  # (kept)\n",
    "    fig.savefig(os.path.join(save_dir, \"all_sources_panel.png\"), dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    all_data: List[Tuple[NewsSource, pd.DataFrame]] = []\n",
    "\n",
    "    for i, source in enumerate(news_sources):\n",
    "        print(f\"Processing {source.key}... {i+1}/{len(news_sources)}\")\n",
    "        try:\n",
    "            df_src = load_source_df(source.key)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ⚠️ Missing file: {source.key}_articles.json — skipping.\")\n",
    "            # Create an empty frame with a sentiment column so downstream code is safe\n",
    "            df_src = pd.DataFrame({'sentiment': []})\n",
    "        all_data.append((source, df_src))\n",
    "        make_panel_for_source(source, df_src, FIG_DIR)\n",
    "\n",
    "    make_panel_cumulative(all_data, FIG_DIR)\n",
    "    print(f\"✅ Done. Saved {len(news_sources)} per-source panels + 1 cumulative panel → {os.path.abspath(FIG_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b0940",
   "metadata": {},
   "source": [
    "## 4. Analyze Politics Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e1a35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Single-file multi-source analysis (configurable 2x3 panels)\n",
    "# =========================\n",
    "\n",
    "import os, json, re\n",
    "from urllib.parse import urlparse\n",
    "from typing import List, Tuple, Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "242c5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "BASE_PATH = \"../../outputs/newsapi/\"\n",
    "ARTICLES_PATH = BASE_PATH + \"articles/politics\"\n",
    "ARTICLES_JSON_PATH = f\"{ARTICLES_PATH}/politics_articles.json\"   # ← single JSON file path\n",
    "FIG_DIR = BASE_PATH + \"figures/politics\"                             # output directory\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Choose which plots to draw (1..6). Layout auto-adjusts (max 3 columns).\n",
    "# 1: Author count dist (linear)\n",
    "# 2: Author count dist (log10)\n",
    "# 3: Top-10 authors (with % of total articles)\n",
    "# 4: Article sentiment histogram\n",
    "# 5: Top-10 authors' sentiment (mean±std)\n",
    "# 6: Articles per day (counts by date)\n",
    "PLOTS_TO_DRAW = [1, 2, 3, 4, 5, 6]   # e.g., [1,2,3,4] → 2x2\n",
    "\n",
    "# Author histogram tweaks\n",
    "AUTHOR_HIST_LOGX_LINEAR_TILE = False  # if True, set xscale('log') on tile #1\n",
    "\n",
    "# Author ignore config\n",
    "IGNORE_AUTHORS = {\n",
    "    # case-insensitive, after cleaning\n",
    "    \"reuters\",\n",
    "    \"pa media\",\n",
    "    \"editorial board\",\n",
    "    \"associated press\",\n",
    "    \"field level media\",\n",
    "    \"more\",\n",
    "}\n",
    "IGNORE_AUTHOR_PATTERNS = [\n",
    "    re.compile(r\"\\b(desk|newsroom)\\b\", re.I),\n",
    "]\n",
    "IGNORE_AGENCY_FLAG = True  # ignore authors where authors[].isAgency == True\n",
    "\n",
    "# Binning for sentiments (display summary percentages)\n",
    "NEG_TH = -0.1\n",
    "POS_TH =  0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84f8306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- AUTHOR EXTRACTION ----------------\n",
    "_SPLIT = re.compile(r'\\s*(?:,| and | & |;|\\|)\\s*', re.IGNORECASE)\n",
    "\n",
    "def _split_tokens(s: str) -> List[str]:\n",
    "    return [t for t in _SPLIT.split(str(s) if s is not None else \"\") if t and t.strip()]\n",
    "\n",
    "def _clean_name(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r'^\\s*by\\s+', '', s, flags=re.IGNORECASE)  # drop leading \"By \"\n",
    "    low = s.lower()\n",
    "    if \"http://\" in low or \"https://\" in low or \".com\" in low or \".co.\" in low:\n",
    "        return \"\"\n",
    "    if \"@\" in s:\n",
    "        s = s.split(\"@\")[0].replace(\".\", \" \").strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def _is_ignored_author(name: str, author_obj: Optional[Dict[str, Any]] = None) -> bool:\n",
    "    if not name:\n",
    "        return True\n",
    "    if author_obj and IGNORE_AGENCY_FLAG:\n",
    "        try:\n",
    "            if bool(author_obj.get(\"isAgency\")):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    low = name.lower()\n",
    "    if low in IGNORE_AUTHORS:\n",
    "        return True\n",
    "    for pat in IGNORE_AUTHOR_PATTERNS:\n",
    "        if pat.search(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def normalize_authors_from_record(rec: Dict[str, Any]) -> List[str]:\n",
    "    names: List[str] = []\n",
    "\n",
    "    # Prefer authors[].name\n",
    "    if isinstance(rec.get('authors'), list):\n",
    "        for a in rec['authors']:\n",
    "            if isinstance(a, dict):\n",
    "                nm = _clean_name(a.get('name', ''))\n",
    "                if nm and not _is_ignored_author(nm, a):\n",
    "                    names.append(nm)\n",
    "            else:\n",
    "                nm = _clean_name(a)\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "\n",
    "    # Fallback: author (str or list)\n",
    "    if not names and ('author' in rec):\n",
    "        auth = rec['author']\n",
    "        if isinstance(auth, list):\n",
    "            for a in auth:\n",
    "                nm = _clean_name(a)\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "        else:\n",
    "            for p in _split_tokens(auth):\n",
    "                nm = _clean_name(p)\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "\n",
    "    # Fallback: byline (str)\n",
    "    if not names and isinstance(rec.get('byline'), str):\n",
    "        for p in _split_tokens(rec['byline']):\n",
    "            nm = _clean_name(p)\n",
    "            if nm and not _is_ignored_author(nm, None):\n",
    "                names.append(nm)\n",
    "\n",
    "    # de-dup, preserve order\n",
    "    seen, out = set(), []\n",
    "    for n in names:\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); out.append(n)\n",
    "    return out\n",
    "\n",
    "# ---------------- LOAD, MAP, NORMALIZE ----------------\n",
    "def load_all_articles(json_path: str) -> pd.DataFrame:\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict) and \"articles\" in data:\n",
    "        data = data[\"articles\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    # standardize expected columns\n",
    "    if 'sentiment' not in df.columns:\n",
    "        df['sentiment'] = np.nan\n",
    "    df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce').clip(-1, 1)\n",
    "    return df\n",
    "\n",
    "# Build domain lookup\n",
    "def _norm_domain(d: str) -> str:\n",
    "    d = (d or \"\").lower()\n",
    "    if d.startswith(\"www.\"): d = d[4:]\n",
    "    return d\n",
    "\n",
    "DOMAIN_TO_SOURCE: Dict[str, NewsSource] = { _norm_domain(ns.domain): ns for ns in news_sources }\n",
    "\n",
    "def map_article_to_source(rec: Dict[str, Any]) -> Optional[NewsSource]:\n",
    "    # 1) try source.uri\n",
    "    s_uri = None\n",
    "    if isinstance(rec.get(\"source\"), dict):\n",
    "        s_uri = rec[\"source\"].get(\"uri\")\n",
    "    if s_uri:\n",
    "        ns = DOMAIN_TO_SOURCE.get(_norm_domain(s_uri))\n",
    "        if ns: return ns\n",
    "\n",
    "    # 2) try URL domain\n",
    "    url = rec.get(\"url\", \"\")\n",
    "    try:\n",
    "        netloc = urlparse(url).netloc.lower()\n",
    "        if netloc.startswith(\"www.\"): netloc = netloc[4:]\n",
    "        # exact or endswith match\n",
    "        if netloc in DOMAIN_TO_SOURCE:\n",
    "            return DOMAIN_TO_SOURCE[netloc]\n",
    "        for dom, ns in DOMAIN_TO_SOURCE.items():\n",
    "            if netloc.endswith(dom):\n",
    "                return ns\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None  # unknown / skip\n",
    "\n",
    "def attach_source_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    mapped = df.apply(lambda r: map_article_to_source(r.to_dict()), axis=1)\n",
    "    df[\"__source_key__\"]  = mapped.apply(lambda ns: ns.key if ns else None)\n",
    "    df[\"__source_name__\"] = mapped.apply(lambda ns: ns.name if ns else None)\n",
    "    df[\"__source_rating__\"] = mapped.apply(lambda ns: ns.mbfc_rating if ns else None)\n",
    "    return df\n",
    "\n",
    "def add_author_list(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"authors_norm\"] = df.apply(lambda r: normalize_authors_from_record(r.to_dict()), axis=1)\n",
    "    df[\"has_author\"] = df[\"authors_norm\"].apply(lambda lst: bool(lst))\n",
    "    return df\n",
    "\n",
    "# ---------------- STATS HELPERS ----------------\n",
    "def author_counts_for_source(df_src: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Per-author article counts = number of *articles* where the author appears at least once.\n",
    "    \"\"\"\n",
    "    if df_src.empty:\n",
    "        return pd.Series(dtype=int)\n",
    "    # explode to author-article rows\n",
    "    ex = df_src.explode(\"authors_norm\")\n",
    "    ex = ex[ex[\"authors_norm\"].notna()]\n",
    "    # Use article \"uri\" as identity; fallback to \"url\"\n",
    "    if \"uri\" not in ex.columns:\n",
    "        ex[\"uri\"] = ex[\"url\"].fillna(\"\")\n",
    "    # groupby author, count unique articles\n",
    "    counts = ex.groupby(\"authors_norm\")[\"uri\"].nunique().sort_values(ascending=False)\n",
    "    return counts\n",
    "\n",
    "def author_hist_bins(counts_per_author: pd.Series) -> np.ndarray:\n",
    "    if counts_per_author.empty:\n",
    "        return np.array([])\n",
    "    max_k = int(counts_per_author.max())\n",
    "    if max_k <= 40:\n",
    "        return np.arange(0.5, max_k + 1.5, 1.0)\n",
    "    return np.linspace(1, max_k, 30)\n",
    "\n",
    "def sent_summary(s: pd.Series) -> Dict[str, float]:\n",
    "    s = pd.to_numeric(s, errors='coerce').dropna().clip(-1, 1)\n",
    "    if s.empty:\n",
    "        return dict(mean=np.nan, std=np.nan, pos=0.0, neu=1.0, neg=0.0)\n",
    "    neg = (s < NEG_TH).mean()\n",
    "    neu = ((s >= NEG_TH) & (s <= POS_TH)).mean()\n",
    "    pos = (s > POS_TH).mean()\n",
    "    return dict(\n",
    "        mean=float(s.mean()),\n",
    "        std=float(s.std(ddof=1)) if len(s) > 1 else 0.0,\n",
    "        pos=float(pos), neu=float(neu), neg=float(neg),\n",
    "    )\n",
    "\n",
    "def per_author_sentiment(df_src: pd.DataFrame, authors: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a given source df and a list of authors (Top-10), compute per-author sentiment mean & std.\n",
    "    \"\"\"\n",
    "    if df_src.empty or not authors:\n",
    "        return pd.DataFrame(columns=[\"author\", \"mean\", \"std\"])\n",
    "    ex = df_src.explode(\"authors_norm\")\n",
    "    ex = ex[ex[\"authors_norm\"].notna()]\n",
    "    ex = ex[ex[\"authors_norm\"].isin(authors)]\n",
    "    ex[\"sentiment\"] = pd.to_numeric(ex[\"sentiment\"], errors=\"coerce\").clip(-1, 1)\n",
    "    g = ex.groupby(\"authors_norm\")[\"sentiment\"]\n",
    "    out = pd.DataFrame({\n",
    "        \"author\": g.mean().index,\n",
    "        \"mean\": g.mean().values,\n",
    "        \"std\": g.std(ddof=1).fillna(0.0).values,\n",
    "        \"n\": g.count().values\n",
    "    }).sort_values(\"mean\", ascending=False)\n",
    "    return out\n",
    "\n",
    "def daily_counts(df_src: pd.DataFrame) -> pd.Series:\n",
    "    if df_src.empty:\n",
    "        return pd.Series(dtype=int)\n",
    "    # prefer dateTimePub, fallback to date\n",
    "    dt = pd.to_datetime(df_src.get(\"dateTimePub\", pd.NaT), errors=\"coerce\", utc=True)\n",
    "    if dt.isna().all():\n",
    "        dt = pd.to_datetime(df_src.get(\"date\", pd.NaT), errors=\"coerce\", utc=True)\n",
    "    days = dt.dt.tz_convert(\"UTC\").dt.date\n",
    "    vc = pd.Series(days, name=\"d\").value_counts().sort_index()\n",
    "    vc.index = pd.to_datetime(vc.index)\n",
    "    return vc\n",
    "\n",
    "# ---------------- PLOTTING ----------------\n",
    "def _add_counts_on_bars(ax):\n",
    "    for p in ax.patches:\n",
    "        w = p.get_width()\n",
    "        if w > 0:\n",
    "            ax.annotate(f'{int(w)}', (w, p.get_y()+p.get_height()/2),\n",
    "                        ha='left', va='center', fontsize=9, xytext=(3,0), textcoords='offset points')\n",
    "\n",
    "def _grid_for_n(n: int, max_cols: int = 3) -> Tuple[int, int]:\n",
    "    cols = min(max_cols, max(1, n))\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    # If exactly 4, make 2x2\n",
    "    if n == 4:\n",
    "        return 2, 2\n",
    "    return rows, cols\n",
    "\n",
    "def plot_panel_for_source(ns: NewsSource, df_src: pd.DataFrame, plots_to_draw: List[int], save_dir: str):\n",
    "    # Base stats\n",
    "    total_articles = int(len(df_src))\n",
    "    counts_per_author = author_counts_for_source(df_src)  # Series: author -> #articles\n",
    "    bins_auth = author_hist_bins(counts_per_author)\n",
    "    mu_auth = float(counts_per_author.mean()) if not counts_per_author.empty else np.nan\n",
    "    sd_auth = float(counts_per_author.std(ddof=1)) if len(counts_per_author) > 1 else (0.0 if not counts_per_author.empty else np.nan)\n",
    "\n",
    "    # sentiment summary (overall)\n",
    "    s_sum = sent_summary(df_src[\"sentiment\"] if \"sentiment\" in df_src.columns else pd.Series([], dtype=float))\n",
    "\n",
    "    # which plots\n",
    "    plot_ids = [p for p in plots_to_draw if p in {1,2,3,4,5,6}]\n",
    "    n = len(plot_ids)\n",
    "    if n == 0:\n",
    "        print(f\"[{ns.key}] Nothing selected to plot.\")\n",
    "        return\n",
    "\n",
    "    rows, cols = _grid_for_n(n, max_cols=3)\n",
    "    fig = plt.figure(figsize=(6*cols+2, 4.5*rows+1), constrained_layout=True)\n",
    "    grid = fig.add_gridspec(rows, cols)\n",
    "\n",
    "    def _next_ax(i):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        return fig.add_subplot(grid[r, c])\n",
    "\n",
    "    i_ax = 0\n",
    "\n",
    "    # 1) Author count dist (linear)\n",
    "    if 1 in plot_ids:\n",
    "        ax = _next_ax(i_ax); i_ax += 1\n",
    "        if not counts_per_author.empty and bins_auth.size > 0:\n",
    "            nvals, bins, patches = ax.hist(counts_per_author.values, bins=bins_auth, edgecolor='black')\n",
    "            ax.set_xlabel(\"Articles per author\")\n",
    "            ax.set_ylabel(\"# of authors\")\n",
    "            ax.set_title(f\"{ns.name} — Author→Article Count (linear)\")\n",
    "            if len(nvals) > 0:\n",
    "                modal_idx = int(np.argmax(nvals))\n",
    "                if 0 <= modal_idx < len(patches):\n",
    "                    patches[modal_idx].set_facecolor('tab:orange')\n",
    "            if not np.isnan(mu_auth):\n",
    "                ax.axvline(mu_auth, linestyle='-', linewidth=2)\n",
    "            if AUTHOR_HIST_LOGX_LINEAR_TILE:\n",
    "                ax.set_xscale('log')\n",
    "            ax.text(0.98, 0.95,\n",
    "                    f\"Articles: {total_articles}\\nμ={mu_auth:.2f}\\nσ={sd_auth:.2f}\",\n",
    "                    ha='right', va='top', transform=ax.transAxes, fontsize=9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "        else:\n",
    "            ax.text(0.5,0.5,\"No author data\", ha='center', va='center')\n",
    "            ax.set_title(f\"Author→Article Count (linear)\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    # 2) Author count dist (log10)\n",
    "    if 2 in plot_ids:\n",
    "        ax = _next_ax(i_ax); i_ax += 1\n",
    "        if not counts_per_author.empty:\n",
    "            vals = counts_per_author.values.astype(float)\n",
    "            vals = vals[vals > 0]\n",
    "            if len(vals):\n",
    "                log_vals = np.log10(vals)\n",
    "                try:\n",
    "                    bins = np.histogram_bin_edges(log_vals, bins='auto')\n",
    "                except Exception:\n",
    "                    bins = np.linspace(log_vals.min(), log_vals.max(), 20)\n",
    "                nvals, b, patches = ax.hist(log_vals, bins=bins, edgecolor='black')\n",
    "                mu_log = float(log_vals.mean())\n",
    "                sd_log = float(log_vals.std(ddof=1)) if len(log_vals) > 1 else 0.0\n",
    "                if len(nvals) > 0:\n",
    "                    modal_idx = int(np.argmax(nvals))\n",
    "                    if 0 <= modal_idx < len(patches):\n",
    "                        patches[modal_idx].set_facecolor('tab:orange')\n",
    "                ax.axvline(mu_log, linestyle='-', linewidth=2)\n",
    "                ax.set_xlabel(\"log10(articles per author)\")\n",
    "                ax.set_ylabel(\"# of authors\")\n",
    "                ax.set_title(f\"Author Count (log10)\")\n",
    "            else:\n",
    "                ax.text(0.5,0.5,\"No author data\", ha='center', va='center')\n",
    "                ax.set_title(f\"Author Count (log10)\")\n",
    "                ax.set_xticks([]); ax.set_yticks([])\n",
    "        else:\n",
    "            ax.text(0.5,0.5,\"No author data\", ha='center', va='center')\n",
    "            ax.set_title(f\"Author Count (log10)\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    # 3) Top-10 authors with % of articles\n",
    "    if 3 in plot_ids:\n",
    "        ax = _next_ax(i_ax); i_ax += 1\n",
    "        if not counts_per_author.empty:\n",
    "            top10 = counts_per_author.head(10)[::-1]  # reversed for barh\n",
    "            if total_articles > 0:\n",
    "                perc = (top10 / total_articles * 100.0).round(1)\n",
    "            else:\n",
    "                perc = pd.Series([0]*len(top10), index=top10.index)\n",
    "            labels = [f\"{a}  ({c} • {p:.1f}%)\" for a, c, p in zip(top10.index, top10.values, perc.values)]\n",
    "            ax.barh(range(len(top10)), top10.values)\n",
    "            ax.set_yticks(range(len(top10))); ax.set_yticklabels(labels)\n",
    "            ax.set_xlabel(\"# of articles (author present)\")\n",
    "            ax.set_title(f\"Top 10 Authors (share of articles)\")\n",
    "        else:\n",
    "            ax.text(0.5,0.5,\"No authors found\", ha='center', va='center')\n",
    "            ax.set_title(f\"Top 10 Authors\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    # 4) Article sentiment distribution\n",
    "    if 4 in plot_ids:\n",
    "        ax = _next_ax(i_ax); i_ax += 1\n",
    "        s = pd.to_numeric(df_src[\"sentiment\"], errors=\"coerce\").dropna().clip(-1,1) if \"sentiment\" in df_src else pd.Series([], dtype=float)\n",
    "        if not s.empty:\n",
    "            n_s, bins_s, patches_s = ax.hist(s, bins=21, edgecolor='black')\n",
    "            ax.set_xlim(-1, 1)\n",
    "            ax.set_xlabel(\"Sentiment [-1,1]\")\n",
    "            ax.set_ylabel(\"# of articles\")\n",
    "            ax.set_title(f\"Sentiment Distribution\")\n",
    "            ax.axvline(0, linestyle='--', linewidth=1)\n",
    "            if not np.isnan(s_sum[\"mean\"]):\n",
    "                ax.axvline(s_sum[\"mean\"], linestyle='-', linewidth=2)\n",
    "            ax.text(0.98, 0.95,\n",
    "                    f\"μ={s_sum['mean']:.3f}\\nσ={s_sum['std']:.3f}\\n\"\n",
    "                    f\"pos>{POS_TH}: {s_sum['pos']*100:.1f}%\\n\"\n",
    "                    f\"{NEG_TH}≤neu≤{POS_TH}: {s_sum['neu']*100:.1f}%\\n\"\n",
    "                    f\"neg<{NEG_TH}: {s_sum['neg']*100:.1f}%\",\n",
    "                    ha='right', va='top', transform=ax.transAxes, fontsize=9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "        else:\n",
    "            ax.text(0.5,0.5,\"No sentiment data\", ha='center', va='center')\n",
    "            ax.set_title(f\"Sentiment Distribution\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    # 5) Top-10 authors' sentiment (mean±std)\n",
    "    if 5 in plot_ids:\n",
    "        ax = _next_ax(i_ax); i_ax += 1\n",
    "        if not counts_per_author.empty and \"sentiment\" in df_src.columns:\n",
    "            top_authors = counts_per_author.head(10).index.tolist()\n",
    "            df_auth_sent = per_author_sentiment(df_src, top_authors)\n",
    "            if not df_auth_sent.empty:\n",
    "                df_auth_sent = df_auth_sent.set_index(\"author\").loc[top_authors]  # keep same order\n",
    "                y = np.arange(len(df_auth_sent))[::-1]\n",
    "                means = df_auth_sent[\"mean\"].values[::-1]\n",
    "                stds  = df_auth_sent[\"std\"].values[::-1]\n",
    "                labels = df_auth_sent.index.tolist()[::-1]\n",
    "\n",
    "                ax.barh(y, means, xerr=stds, capsize=3, alpha=0.9)\n",
    "                ax.set_yticks(y); ax.set_yticklabels(labels)\n",
    "                ax.set_xlim(-1, 1)\n",
    "                ax.axvline(0, linestyle='--', linewidth=1)\n",
    "                ax.set_xlabel(\"Sentiment mean (± std)\")\n",
    "                ax.set_title(f\"Top 10 Authors: Sentiment μ±σ\")\n",
    "            else:\n",
    "                ax.text(0.5,0.5,\"No sentiment for top authors\", ha='center', va='center')\n",
    "                ax.set_title(f\"Top 10 Authors: Sentiment μ±σ\")\n",
    "                ax.set_xticks([]); ax.set_yticks([])\n",
    "        else:\n",
    "            ax.text(0.5,0.5,\"No data for top authors\", ha='center', va='center')\n",
    "            ax.set_title(f\"Top 10 Authors: Sentiment μ±σ\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    # 6) Articles per day\n",
    "    if 6 in plot_ids:\n",
    "        ax = _next_ax(i_ax); i_ax += 1\n",
    "        dc = daily_counts(df_src)\n",
    "        if not dc.empty:\n",
    "            ax.bar(dc.index, dc.values, width=0.85)\n",
    "            ax.set_xlabel(\"Date\")\n",
    "            ax.set_ylabel(\"# of articles\")\n",
    "            ax.set_title(f\"Articles per Day\")\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            # add small stats legend\n",
    "            ax.text(0.98, 0.95,\n",
    "                    f\"Total days: {len(dc)}\\nTotal articles: {int(dc.sum())}\\nμ/day={dc.mean():.2f}\\nσ/day={dc.std(ddof=1):.2f}\",\n",
    "                    ha='right', va='top', transform=ax.transAxes, fontsize=9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "        else:\n",
    "            ax.text(0.5,0.5,\"No dates\", ha='center', va='center')\n",
    "            ax.set_title(f\"Articles per Day\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    fig.suptitle(f\"{ns.name}\", fontsize=14)  # keep header wording consistent\n",
    "    out_path = os.path.join(save_dir, f\"{ns.key}_panel.png\")\n",
    "    fig.savefig(out_path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run_all(json_path: str, save_dir: str, plots_to_draw: List[int], also_cumulative: bool = True):\n",
    "    df = load_all_articles(json_path)\n",
    "    df = attach_source_columns(df)\n",
    "    df = add_author_list(df)\n",
    "\n",
    "    # Only keep rows mapped to our known sources\n",
    "    df = df[df[\"__source_key__\"].notna()].copy()\n",
    "    if df.empty:\n",
    "        print(\"No rows matched to known sources.\")\n",
    "        return\n",
    "\n",
    "    # Per-source panels\n",
    "    for ns in news_sources:\n",
    "        df_src = df[df[\"__source_key__\"] == ns.key].copy()\n",
    "        if df_src.empty:\n",
    "            continue\n",
    "        print(f\"Plotting {ns.name} ({len(df_src)} articles)\")\n",
    "        plot_panel_for_source(ns, df_src, plots_to_draw, save_dir)\n",
    "\n",
    "    # Optional: cumulative panel (uses same selected plots)\n",
    "    if also_cumulative:\n",
    "        # Build a pseudo-NewsSource for \"All\"\n",
    "        ns_all = NewsSource(\"all_sources\", \"—\", \"All Sources\", \"—\")\n",
    "        plot_panel_for_source(ns_all, df.copy(), plots_to_draw, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37c7b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Fox News (1858 articles)\n",
      "Plotting Breitbart (1368 articles)\n",
      "Plotting Daily Mail (5807 articles)\n",
      "Plotting The Sun (463 articles)\n",
      "Plotting Wall Street Journal (127 articles)\n",
      "Plotting New York Post (1487 articles)\n",
      "Plotting Forbes (366 articles)\n",
      "Plotting India Times (187 articles)\n",
      "Plotting Newsweek (1370 articles)\n",
      "Plotting Reuters (945 articles)\n",
      "Plotting The Hill (1927 articles)\n",
      "Plotting The New York Times (1231 articles)\n",
      "Plotting The Washington Post (266 articles)\n",
      "Plotting USA Today (437 articles)\n",
      "Plotting CBS News (2081 articles)\n",
      "Plotting SF Gate (135 articles)\n",
      "Plotting Bloomberg (1058 articles)\n",
      "Plotting People (182 articles)\n"
     ]
    }
   ],
   "source": [
    "# ---------------- RUN ----------------\n",
    "run_all(ARTICLES_JSON_PATH, FIG_DIR, PLOTS_TO_DRAW, also_cumulative=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
