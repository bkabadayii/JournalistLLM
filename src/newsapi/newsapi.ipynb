{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d45323b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NEWSAPI_API_KEY = os.getenv(\"NEWSAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5e42d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSource:\n",
    "    def __init__(self, key: str, domain: str, mbfc_rating: str):\n",
    "        self.key = key\n",
    "        self.domain = domain\n",
    "        self.mbfc_rating = mbfc_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53a34e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init all news sources\n",
    "\"\"\"\n",
    "- Right\n",
    "    - Fox News - [foxnews.com](http://foxnews.com)\n",
    "    - Breitbart - [breitbart.com](http://breitbart.com)\n",
    "    - Dailymail - [dailymail.co.uk](http://dailymail.co.uk) → UK\n",
    "    - The Sun - [thesun.co.uk](http://thesun.co.uk/) → UK\n",
    "- Right-Center\n",
    "    - Wall Street Journal - [wsj.com](http://wsj.com)\n",
    "    - New York Post - [nypost.com](http://nypost.com/)\n",
    "    - Forbes - [forbes.com](http://forbes.com)\n",
    "    - India Times - [indiatimes.com](http://indiatimes.com) → India\n",
    "    - News Week - [newsweek.com](http://newsweek.com/)\n",
    "- Neutral\n",
    "    - Reuters - [reuters.com](http://reuters.com)\n",
    "    - The Hill - [thehill.com](http://thehill.com)\n",
    "- Left-Center\n",
    "    - New York Times - [nytimes.com](http://nytimes.com)\n",
    "    - Washington Post - [washingtonpost.com](http://washingtonpost.com)\n",
    "    - USA Today - [usatoday.com](http://usatoday.com)\n",
    "    - Buzz Feed - [buzzfeed.com](http://buzzfeed.com)\n",
    "    - CBS News - [cbsnews.com](http://cbsnews.com)\n",
    "    - SF Gate - [sfgate.com](http://sfgate.com/)\n",
    "    - Bloomberg - [bloomberg.com](http://bloomberg.com)\n",
    "- Left\n",
    "    - CNN - [cnn.com](http://cnn.com)\n",
    "    - People - [people.com](http://people.com)\n",
    "\"\"\"\n",
    "news_sources = [\n",
    "    # Right\n",
    "    NewsSource(\"foxnews\", \"foxnews.com\", \"Right\"),\n",
    "    NewsSource(\"breitbart\", \"breitbart.com\", \"Right\"),\n",
    "    NewsSource(\"dailymail\", \"dailymail.co.uk\", \"Right\"),\n",
    "    NewsSource(\"thesun\", \"thesun.co.uk\", \"Right\"),\n",
    "    # Right-Center\n",
    "    NewsSource(\"wsj\", \"wsj.com\", \"Right-Center\"),\n",
    "    NewsSource(\"nypost\", \"nypost.com\", \"Right-Center\"),\n",
    "    NewsSource(\"forbes\", \"forbes.com\", \"Right-Center\"),\n",
    "    NewsSource(\"indiatimes\", \"indiatimes.com\", \"Right-Center\"),\n",
    "    NewsSource(\"newsweek\", \"newsweek.com\", \"Right-Center\"),\n",
    "    # Neutral\n",
    "    NewsSource(\"reuters\", \"reuters.com\", \"Neutral\"),\n",
    "    NewsSource(\"thehill\", \"thehill.com\", \"Neutral\"),\n",
    "    # Left-Center\n",
    "    NewsSource(\"nytimes\", \"nytimes.com\", \"Left-Center\"),\n",
    "    NewsSource(\"washingtonpost\", \"washingtonpost.com\", \"Left-Center\"),\n",
    "    NewsSource(\"usatoday\", \"usatoday.com\", \"Left-Center\"),\n",
    "    NewsSource(\"buzzfeed\", \"buzzfeed.com\", \"Left-Center\"),\n",
    "    NewsSource(\"cbsnews\", \"cbsnews.com\", \"Left-Center\"),\n",
    "    NewsSource(\"sfgate\", \"sfgate.com\", \"Left-Center\"),\n",
    "    NewsSource(\"bloomberg\", \"bloomberg.com\", \"Left-Center\"),\n",
    "    # Left\n",
    "    NewsSource(\"cnn\", \"cnn.com\", \"Left\"),\n",
    "    NewsSource(\"people\", \"people.com\", \"Left\"),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ce3f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../../outputs/newsapi/\"\n",
    "ARTICLES_PATH = BASE_PATH + \"articles/\"\n",
    "AUTHORS_PATH = BASE_PATH + \"authors/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd7612",
   "metadata": {},
   "source": [
    "### Fetch The Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import EventRegistry, QueryArticlesIter\n",
    "er = EventRegistry(apiKey=NEWSAPI_API_KEY)\n",
    "max_articles = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in news_sources:\n",
    "    q = QueryArticlesIter(\n",
    "        sourceUri = source.domain,\n",
    "        lang = \"eng\",\n",
    "        dateStart = \"2025-01-01\",\n",
    "    )\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for art in q.execQuery(er, sortBy = \"socialScore\", maxItems = 10000):\n",
    "        articles.append(art)\n",
    "    \n",
    "    # store the articles in a json file\n",
    "    with open(f\"{ARTICLES_PATH}{source.key}_articles.json\", \"w\") as f:\n",
    "        json.dump(articles, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbfdd8",
   "metadata": {},
   "source": [
    "### Analyze Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ece092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9be0f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing foxnews... 1/20\n",
      "Processing breitbart... 2/20\n",
      "Processing dailymail... 3/20\n",
      "Processing thesun... 4/20\n",
      "Processing wsj... 5/20\n",
      "Processing nypost... 6/20\n",
      "Processing forbes... 7/20\n",
      "Processing indiatimes... 8/20\n",
      "Processing newsweek... 9/20\n",
      "Processing reuters... 10/20\n",
      "Processing thehill... 11/20\n",
      "Processing nytimes... 12/20\n",
      "Processing washingtonpost... 13/20\n",
      "Processing usatoday... 14/20\n",
      "Processing buzzfeed... 15/20\n",
      "Processing cbsnews... 16/20\n",
      "Processing sfgate... 17/20\n",
      "Processing bloomberg... 18/20\n",
      "Processing cnn... 19/20\n",
      "Processing people... 20/20\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for i, source in enumerate(news_sources):\n",
    "    print(f\"Processing {source.key}... {i+1}/{len(news_sources)}\")\n",
    "    # Load the articles\n",
    "    with open(f\"{ARTICLES_PATH}{source.key}_articles.json\", \"r\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    authors2count = {}\n",
    "\n",
    "    for article in articles:\n",
    "        authors = article.get(\"authors\", [])\n",
    "        for author in authors:\n",
    "            author_name = author.get(\"name\", \"\")\n",
    "            if author_name in authors2count:\n",
    "                authors2count[author_name][\"count\"] += 1\n",
    "            else:\n",
    "                authors2count[author_name] = {\n",
    "                    \"name\": author_name,\n",
    "                    \"count\": 1,\n",
    "                    \"source\": source.key,\n",
    "                    \"mbfc_rating\": source.mbfc_rating,\n",
    "                    \"percentage\": 0\n",
    "                }\n",
    "\n",
    "    # Calculate the percentage\n",
    "    total_count = sum(author[\"count\"] for author in authors2count.values())\n",
    "    for author in authors2count:\n",
    "        authors2count[author][\"percentage\"] = authors2count[author][\"count\"] / total_count\n",
    "\n",
    "    # Sort the authors by count\n",
    "    authors2count = sorted(authors2count.values(), key=lambda x: x[\"count\"], reverse=True)\n",
    "\n",
    "    # Write the results to a csv file\n",
    "    authors_df = pd.DataFrame(authors2count)\n",
    "    authors_df.to_csv(f\"{AUTHORS_PATH}{source.key}_authors.csv\", index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "351733fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for forbes[forbes.com]\n",
      "No articles found for usatoday[usatoday.com]\n",
      "No articles found for buzzfeed[buzzfeed.com]\n",
      "No articles found for cnn[cnn.com]\n"
     ]
    }
   ],
   "source": [
    "# Merge all the authors csv files (sorted by percentage)\n",
    "merged_authors_df = pd.DataFrame()\n",
    "for source in news_sources:\n",
    "    try: \n",
    "        authors_df = pd.read_csv(f\"{AUTHORS_PATH}{source.key}_authors.csv\")\n",
    "        merged_authors_df = pd.concat([merged_authors_df, authors_df])\n",
    "    except:\n",
    "        print(f\"No articles found for {source.key}[{source.domain}]\")\n",
    "\n",
    "merged_authors_df = merged_authors_df.sort_values(by=\"percentage\", ascending=False)\n",
    "merged_authors_df.to_csv(f\"{AUTHORS_PATH}all_authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f16977",
   "metadata": {},
   "source": [
    "### Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d570b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing foxnews... 1/20\n",
      "Processing breitbart... 2/20\n",
      "Processing dailymail... 3/20\n",
      "Processing thesun... 4/20\n",
      "Processing wsj... 5/20\n",
      "Processing nypost... 6/20\n",
      "Processing forbes... 7/20\n",
      "Processing indiatimes... 8/20\n",
      "Processing newsweek... 9/20\n",
      "Processing reuters... 10/20\n",
      "Processing thehill... 11/20\n",
      "Processing nytimes... 12/20\n",
      "Processing washingtonpost... 13/20\n",
      "Processing usatoday... 14/20\n",
      "Processing buzzfeed... 15/20\n",
      "Processing cbsnews... 16/20\n",
      "Processing sfgate... 17/20\n",
      "Processing bloomberg... 18/20\n",
      "Processing cnn... 19/20\n",
      "Processing people... 20/20\n",
      "✅ Done. Saved 20 per-source panels + 1 cumulative panel → /Users/baturalpkabadayi/Dev/Research/JournalistLLM/outputs/newsapi/figures\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# News distributions panels\n",
    "# =========================\n",
    "# Requirements: pandas, matplotlib, numpy\n",
    "\n",
    "import os, json, re\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FIG_DIR = BASE_PATH + \"figures\"              # where to save the PNGs\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Toggle if you want a log-scale x-axis for the author histogram (useful when long-tailed)\n",
    "AUTHOR_HIST_LOGX = False\n",
    "\n",
    "# Authors to ignore (case-insensitive). After cleaning, if an article's authors are all ignored, it counts as \"no author\".\n",
    "IGNORE_AUTHORS = {\n",
    "    \"reuters\",\n",
    "    \"pa media\",\n",
    "    \"editorial board\",\n",
    "    \"associated press\",\n",
    "    \"field level media\",\n",
    "    \"more\",\n",
    "\n",
    "}\n",
    "# Regex patterns to ignore (case-insensitive), applied to the cleaned name.\n",
    "IGNORE_AUTHOR_PATTERNS = [\n",
    "    re.compile(r\"\\b(desk|newsroom)\\b\", re.I),\n",
    "]\n",
    "# If True, drop authors where authors[].isAgency == True (when present in JSON).\n",
    "IGNORE_AGENCY_FLAG = True\n",
    "\n",
    "RATING_ORDER = [\"Right\", \"Right-Center\", \"Neutral\", \"Left-Center\", \"Left\"]\n",
    "\n",
    "# ---------- AUTHOR EXTRACTION ----------\n",
    "_SPLIT = re.compile(r'\\s*(?:,| and | & |;|\\|)\\s*', re.IGNORECASE)\n",
    "\n",
    "def _split_tokens(s: str) -> List[str]:\n",
    "    return [t for t in _SPLIT.split(s) if t and t.strip()]\n",
    "\n",
    "def _clean_name(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # Remove \"By ...\" prefix\n",
    "    s = re.sub(r'^\\s*by\\s+', '', s, flags=re.IGNORECASE)\n",
    "    low = s.lower()\n",
    "    # Drop obvious junk\n",
    "    if \"http://\" in low or \"https://\" in low or \".com\" in low or \".co.\" in low:\n",
    "        return \"\"\n",
    "    # Convert emails to a readable token (left side)\n",
    "    if \"@\" in s:\n",
    "        s = s.split(\"@\")[0].replace(\".\", \" \").strip()\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def _is_ignored_author(name: str, author_obj: Dict[str, Any] | None = None) -> bool:\n",
    "    if not name:\n",
    "        return True\n",
    "    if author_obj and IGNORE_AGENCY_FLAG:\n",
    "        try:\n",
    "            if bool(author_obj.get(\"isAgency\")):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    low = name.lower()\n",
    "    if low in IGNORE_AUTHORS:\n",
    "        return True\n",
    "    for pat in IGNORE_AUTHOR_PATTERNS:\n",
    "        if pat.search(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def normalize_authors_from_record(rec: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preferred: rec['authors'] = [{name, isAgency, ...}, ...]\n",
    "    Fallbacks: rec['author'] (str or list), rec['byline'] (str)\n",
    "    Returns a de-duplicated list of cleaned author names after applying ignore rules.\n",
    "    \"\"\"\n",
    "    names: List[str] = []\n",
    "\n",
    "    # Primary: authors[].name\n",
    "    if isinstance(rec.get('authors'), list):\n",
    "        for a in rec['authors']:\n",
    "            if isinstance(a, dict):\n",
    "                nm = _clean_name(a.get('name', ''))\n",
    "                if nm and not _is_ignored_author(nm, a):\n",
    "                    names.append(nm)\n",
    "            else:\n",
    "                nm = _clean_name(str(a))\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "\n",
    "    # Fallback: author (str or list)\n",
    "    if not names and ('author' in rec):\n",
    "        auth = rec['author']\n",
    "        if isinstance(auth, list):\n",
    "            for a in auth:\n",
    "                nm = _clean_name(str(a))\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "        else:\n",
    "            for p in _split_tokens(str(auth)):\n",
    "                nm = _clean_name(p)\n",
    "                if nm and not _is_ignored_author(nm, None):\n",
    "                    names.append(nm)\n",
    "\n",
    "    # Fallback: byline (str)\n",
    "    if not names and isinstance(rec.get('byline'), str):\n",
    "        for p in _split_tokens(rec['byline']):\n",
    "            nm = _clean_name(p)\n",
    "            if nm and not _is_ignored_author(nm, None):\n",
    "                names.append(nm)\n",
    "\n",
    "    # de-dup, preserve order\n",
    "    seen, out = set(), []\n",
    "    for n in names:\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); out.append(n)\n",
    "    return out\n",
    "\n",
    "def explode_authors_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['authors_norm'] = df.apply(normalize_authors_from_record, axis=1)\n",
    "    exploded = df.explode('authors_norm', ignore_index=True)\n",
    "    exploded['has_author'] = exploded['authors_norm'].notna()\n",
    "    return exploded\n",
    "\n",
    "# ---------- LOAD & METRICS ----------\n",
    "def load_source_df(source_key: str) -> pd.DataFrame:\n",
    "    fp = os.path.join(ARTICLES_PATH, f\"{source_key}_articles.json\")\n",
    "    with open(fp, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # supports list of records OR {\"articles\":[...]}\n",
    "    if isinstance(data, dict) and 'articles' in data:\n",
    "        data = data['articles']\n",
    "    df = pd.DataFrame(data)\n",
    "    # ensure a sentiment column exists and is numeric in [-1, 1]\n",
    "    if 'sentiment' not in df.columns:\n",
    "        df['sentiment'] = np.nan\n",
    "    df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce').clip(-1, 1)\n",
    "    return df\n",
    "\n",
    "def per_author_article_counts(exploded_author_series: pd.Series) -> pd.Series:\n",
    "    return exploded_author_series.value_counts()\n",
    "\n",
    "def author_hist_bins(counts_per_author: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sensible bins for right-skewed author counts.\n",
    "    - If max <= 40 → integer-centered bins: [0.5, 1.5, ..., max+0.5]\n",
    "    - Else → 30 linear bins [1, max]\n",
    "    \"\"\"\n",
    "    if counts_per_author.empty:\n",
    "        return np.array([])\n",
    "    max_k = int(counts_per_author.max())\n",
    "    if max_k <= 40:\n",
    "        return np.arange(0.5, max_k + 1.5, 1.0)\n",
    "    return np.linspace(1, max_k, 30)\n",
    "\n",
    "def author_count_stats(counts_per_author: pd.Series) -> Tuple[float, float]:\n",
    "    if counts_per_author.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    mu = float(counts_per_author.mean())\n",
    "    sigma = float(counts_per_author.std(ddof=1)) if len(counts_per_author) > 1 else 0.0\n",
    "    return mu, sigma\n",
    "\n",
    "def sentiment_stats(sentiments: pd.Series) -> Dict[str, float]:\n",
    "    s = sentiments.dropna().astype(float).clip(-1, 1)\n",
    "    if s.empty:\n",
    "        return dict(mean=np.nan, std=np.nan, pos=0.0, neu=1.0, neg=0.0)\n",
    "    neg = (s < -0.2).mean()\n",
    "    neu = ((s >= -0.2) & (s <= 0.2)).mean()\n",
    "    pos = (s > 0.2).mean()\n",
    "    return dict(\n",
    "        mean=float(s.mean()),\n",
    "        std=float(s.std(ddof=1)) if len(s) > 1 else 0.0,\n",
    "        pos=float(pos), neu=float(neu), neg=float(neg),\n",
    "    )\n",
    "\n",
    "def _label_hbar_counts(ax):\n",
    "    for p in ax.patches:\n",
    "        w = p.get_width()\n",
    "        if w > 0:\n",
    "            ax.annotate(f'{int(w)}', (w, p.get_y()+p.get_height()/2),\n",
    "                        ha='left', va='center', fontsize=9, xytext=(3,0), textcoords='offset points')\n",
    "\n",
    "# ---------- PANELS ----------\n",
    "def _plot_log_author_hist(ax, counts_per_author: pd.Series, title_prefix: str):\n",
    "    \"\"\"\n",
    "    Bottom-right tile: histogram of log10(articles per author).\n",
    "    Shows μ and σ in log space and highlights modal bin.\n",
    "    \"\"\"\n",
    "    if counts_per_author.empty:\n",
    "        ax.text(0.5, 0.5, \"No author data\", ha='center', va='center')\n",
    "        ax.set_title(f\"{title_prefix} Log Author Count Distribution\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        return\n",
    "\n",
    "    vals = counts_per_author.values.astype(float)\n",
    "    vals = vals[vals > 0]  # safety\n",
    "    log_vals = np.log10(vals)\n",
    "\n",
    "    # bins: auto works well on log data; fall back to 20 if needed\n",
    "    try:\n",
    "        bins = np.histogram_bin_edges(log_vals, bins='auto')\n",
    "    except Exception:\n",
    "        bins = np.linspace(log_vals.min(), log_vals.max(), 20)\n",
    "\n",
    "    n, b, patches = ax.hist(log_vals, bins=bins, edgecolor='black')\n",
    "    # highlight modal bin\n",
    "    if len(n) > 0:\n",
    "        modal_idx = int(np.argmax(n))\n",
    "        if 0 <= modal_idx < len(patches):\n",
    "            patches[modal_idx].set_facecolor('tab:orange')\n",
    "\n",
    "    mu_log = float(log_vals.mean()) if log_vals.size else np.nan\n",
    "    sd_log = float(log_vals.std(ddof=1)) if log_vals.size > 1 else 0.0\n",
    "    if not np.isnan(mu_log):\n",
    "        ax.axvline(mu_log, linestyle='-', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel(\"log10(articles per author)\")\n",
    "    ax.set_ylabel(\"# of authors\")\n",
    "    ax.set_title(f\"{title_prefix} Log Author Count Distribution\")\n",
    "    # annotate μ, σ (log space)\n",
    "    ax.text(0.98, 0.95,\n",
    "            f\"μ(log10)={mu_log:.3f}\\nσ(log10)={sd_log:.3f}\",\n",
    "            ha='right', va='top', transform=ax.transAxes, fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "\n",
    "def make_panel_for_source(source: NewsSource, df: pd.DataFrame, save_dir: str):\n",
    "    # Standardize columns for safety\n",
    "    df = df.copy()\n",
    "    if 'sentiment' not in df.columns:\n",
    "        df['sentiment'] = np.nan\n",
    "    if not any(col in df.columns for col in ('authors','author','byline')):\n",
    "        df['authors'] = [[] for _ in range(len(df))]\n",
    "\n",
    "    total_articles = int(len(df))\n",
    "\n",
    "    # Authors (with ignore filtering)\n",
    "    author_lists = df.apply(normalize_authors_from_record, axis=1)\n",
    "    no_author_count = int((author_lists.apply(len) == 0).sum())\n",
    "\n",
    "    exploded = df.copy()\n",
    "    exploded['authors_norm'] = author_lists\n",
    "    exploded = exploded.explode('authors_norm', ignore_index=True)\n",
    "    exploded['has_author'] = exploded['authors_norm'].notna()\n",
    "\n",
    "    top_authors = (exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "                   .value_counts()\n",
    "                   .head(10))\n",
    "    top10_total = int(top_authors.sum()) if not top_authors.empty else 0\n",
    "\n",
    "    counts_per_author = per_author_article_counts(\n",
    "        exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "    )\n",
    "    mu_auth, sigma_auth = author_count_stats(counts_per_author)\n",
    "    bins_auth = author_hist_bins(counts_per_author)\n",
    "\n",
    "    # Sentiment (JSON only)\n",
    "    s_stats = sentiment_stats(df['sentiment'])\n",
    "    avg_sent, std_sent = s_stats['mean'], s_stats['std']\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(14, 10), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1,1])\n",
    "    ax11 = fig.add_subplot(gs[0,0])   # author count histogram (linear)\n",
    "    ax12 = fig.add_subplot(gs[0,1])   # top 10 authors\n",
    "    ax21 = fig.add_subplot(gs[1,0])   # sentiment histogram\n",
    "    ax22 = fig.add_subplot(gs[1,1])   # log author count histogram (NEW)\n",
    "\n",
    "    # (1,1) Author count histogram (linear x)\n",
    "    if not counts_per_author.empty and bins_auth.size > 0:\n",
    "        n, bins, patches = ax11.hist(counts_per_author.values, bins=bins_auth, edgecolor='black')\n",
    "        ax11.set_xlabel(\"Articles per author\")\n",
    "        ax11.set_ylabel(\"# of authors\")\n",
    "        ax11.set_title(f\"[{source.key}] Author→Article Count Distribution\")\n",
    "        # highlight modal bin\n",
    "        if len(n) > 0:\n",
    "            modal_idx = int(np.argmax(n))\n",
    "            if 0 <= modal_idx < len(patches):\n",
    "                patches[modal_idx].set_facecolor('tab:orange')\n",
    "        # mean line\n",
    "        if not np.isnan(mu_auth):\n",
    "            ax11.axvline(mu_auth, linestyle='-', linewidth=2)\n",
    "        if AUTHOR_HIST_LOGX:\n",
    "            ax11.set_xscale('log')\n",
    "        # annotation\n",
    "        ax11.text(0.98, 0.95,\n",
    "                  f\"Total articles: {total_articles}\\n\"\n",
    "                  f\"No-author articles: {no_author_count}\\n\"\n",
    "                  f\"μ (per-author count): {mu_auth:.2f}\\nσ: {sigma_auth:.2f}\",\n",
    "                  ha='right', va='top', transform=ax11.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax11.text(0.5, 0.5, \"No author data\", ha='center', va='center')\n",
    "        ax11.set_title(f\"[{source.key}] Author→Article Count Distribution\")\n",
    "        ax11.set_xticks([]); ax11.set_yticks([])\n",
    "\n",
    "    # (1,2) Top 10 authors\n",
    "    if not top_authors.empty:\n",
    "        top_authors.iloc[::-1].plot(kind='barh', ax=ax12)\n",
    "        ax12.set_xlabel(\"Article count\")\n",
    "        ax12.set_ylabel(\"Author\")\n",
    "        ax12.set_title(f\"[{source.key}] Top 10 Authors (total={top10_total})\")\n",
    "        # highlight the top bar (rightmost)\n",
    "        if len(ax12.patches) > 0:\n",
    "            ax12.patches[-1].set_facecolor('tab:red')\n",
    "        _label_hbar_counts(ax12)\n",
    "    else:\n",
    "        ax12.text(0.5, 0.5, \"No authors found\", ha='center', va='center')\n",
    "        ax12.set_title(f\"[{source.key}] Top 10 Authors (total=0)\")\n",
    "        ax12.set_xticks([]); ax12.set_yticks([])\n",
    "\n",
    "    # (2,1) Sentiment histogram\n",
    "    s = df['sentiment'].dropna().astype(float).clip(-1,1)\n",
    "    if not s.empty:\n",
    "        n_s, bins_s, patches_s = ax21.hist(s, bins=21, edgecolor='black')\n",
    "        ax21.set_xlim(-1, 1)\n",
    "        ax21.set_xlabel(\"Sentiment (JSON, [-1,1])\")\n",
    "        ax21.set_ylabel(\"# of articles\")\n",
    "        ax21.set_title(f\"[{source.key}] Sentiment Distribution\")\n",
    "        ax21.axvline(0, linestyle='--', linewidth=1)\n",
    "        if not np.isnan(avg_sent):\n",
    "            ax21.axvline(avg_sent, linestyle='-', linewidth=2)\n",
    "        ax21.text(0.98, 0.95,\n",
    "                  (f\"μ={avg_sent:.3f}\\nσ={std_sent:.3f}\\n\"\n",
    "                   f\"pos>{0.2}: {s_stats['pos']*100:.1f}%\\n\"\n",
    "                   f\"-0.2≤neu≤0.2: {s_stats['neu']*100:.1f}%\\n\"\n",
    "                   f\"neg<-0.2: {s_stats['neg']*100:.1f}%\")\n",
    "                  if not np.isnan(avg_sent) else \"μ=NaN\\nσ=NaN\",\n",
    "                  ha='right', va='top', transform=ax21.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax21.text(0.5, 0.5, \"No sentiment data in JSON\", ha='center', va='center')\n",
    "        ax21.set_title(f\"[{source.key}] Sentiment Distribution\")\n",
    "        ax21.set_xticks([]); ax21.set_yticks([])\n",
    "\n",
    "    # (2,2) NEW: Log author count histogram\n",
    "    _plot_log_author_hist(ax22, counts_per_author, f\"[{source.key}]\")\n",
    "\n",
    "    fig.suptitle(f\"{source.key} — Distributions Panel\", fontsize=14)  # (kept exactly as you asked)\n",
    "    out_path = os.path.join(save_dir, f\"{source.key}_panel.png\")\n",
    "    fig.savefig(out_path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "def make_panel_cumulative(all_records: List[Tuple[NewsSource, pd.DataFrame]], save_dir: str):\n",
    "    frames = []\n",
    "    for src, df in all_records:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        tmp = df.copy()\n",
    "        tmp['__rating__'] = src.mbfc_rating\n",
    "        tmp['__source__'] = src.key\n",
    "        # ensure sentiment column exists\n",
    "        if 'sentiment' not in tmp.columns:\n",
    "            tmp['sentiment'] = np.nan\n",
    "        frames.append(tmp)\n",
    "\n",
    "    if not frames:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.text(0.5, 0.5, \"No data across sources\", ha='center', va='center')\n",
    "        fig.savefig(os.path.join(save_dir, \"all_sources_panel.png\"), dpi=160)\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    big = pd.concat(frames, ignore_index=True)\n",
    "    if 'sentiment' not in big.columns:\n",
    "        big['sentiment'] = np.nan\n",
    "\n",
    "    # authors (WITH ignore filtering)\n",
    "    author_lists = big.apply(normalize_authors_from_record, axis=1)\n",
    "    exploded = big.copy()\n",
    "    exploded['authors_norm'] = author_lists\n",
    "    exploded = exploded.explode('authors_norm', ignore_index=True)\n",
    "    exploded['has_author'] = exploded['authors_norm'].notna()\n",
    "\n",
    "    top_authors = (exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "                   .value_counts()\n",
    "                   .head(10))\n",
    "    top10_total = int(top_authors.sum()) if not top_authors.empty else 0\n",
    "\n",
    "    counts_per_author = per_author_article_counts(\n",
    "        exploded.loc[exploded['has_author'], 'authors_norm']\n",
    "    )\n",
    "    mu_auth, sigma_auth = author_count_stats(counts_per_author)\n",
    "    bins_auth = author_hist_bins(counts_per_author)\n",
    "\n",
    "    # sentiment\n",
    "    big['sentiment'] = pd.to_numeric(big['sentiment'], errors='coerce').clip(-1, 1)\n",
    "    s_stats = sentiment_stats(big['sentiment'])\n",
    "    avg_sent, std_sent = s_stats['mean'], s_stats['std']\n",
    "\n",
    "    # plotting\n",
    "    fig = plt.figure(figsize=(14, 10), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1,1])\n",
    "    ax11 = fig.add_subplot(gs[0,0])\n",
    "    ax12 = fig.add_subplot(gs[0,1])\n",
    "    ax21 = fig.add_subplot(gs[1,0])\n",
    "    ax22 = fig.add_subplot(gs[1,1])\n",
    "\n",
    "    # (1,1) cumulative author histogram (linear)\n",
    "    if not counts_per_author.empty and bins_auth.size > 0:\n",
    "        n, bins, patches = ax11.hist(counts_per_author.values, bins=bins_auth, edgecolor='black')\n",
    "        ax11.set_xlabel(\"Articles per author\")\n",
    "        ax11.set_ylabel(\"# of authors\")\n",
    "        ax11.set_title(\"[All] Author→Article Count Distribution\")\n",
    "        if len(n) > 0:\n",
    "            modal_idx = int(np.argmax(n))\n",
    "            if 0 <= modal_idx < len(patches):\n",
    "                patches[modal_idx].set_facecolor('tab:orange')\n",
    "        if not np.isnan(mu_auth):\n",
    "            ax11.axvline(mu_auth, linestyle='-', linewidth=2)\n",
    "        if AUTHOR_HIST_LOGX:\n",
    "            ax11.set_xscale('log')\n",
    "        ax11.text(0.98, 0.95,\n",
    "                  f\"μ (per-author count): {mu_auth:.2f}\\nσ: {sigma_auth:.2f}\",\n",
    "                  ha='right', va='top', transform=ax11.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax11.text(0.5, 0.5, \"No author data\", ha='center', va='center')\n",
    "        ax11.set_title(\"[All] Author→Article Count Distribution\")\n",
    "        ax11.set_xticks([]); ax11.set_yticks([])\n",
    "\n",
    "    # (1,2) cumulative top 10 authors\n",
    "    if not top_authors.empty:\n",
    "        top_authors.iloc[::-1].plot(kind='barh', ax=ax12)\n",
    "        ax12.set_xlabel(\"Article count\")\n",
    "        ax12.set_ylabel(\"Author\")\n",
    "        ax12.set_title(f\"[All] Top 10 Authors (total={top10_total})\")\n",
    "        if len(ax12.patches) > 0:\n",
    "            ax12.patches[-1].set_facecolor('tab:red')\n",
    "        _label_hbar_counts(ax12)\n",
    "    else:\n",
    "        ax12.text(0.5, 0.5, \"No authors found\", ha='center', va='center')\n",
    "        ax12.set_title(f\"[All] Top 10 Authors (total=0)\")\n",
    "        ax12.set_xticks([]); ax12.set_yticks([])\n",
    "\n",
    "    # (2,1) cumulative sentiment histogram\n",
    "    s = big['sentiment'].dropna().astype(float).clip(-1,1)\n",
    "    if not s.empty:\n",
    "        n_s, bins_s, patches_s = ax21.hist(s, bins=21, edgecolor='black')\n",
    "        ax21.set_xlim(-1, 1)\n",
    "        ax21.set_xlabel(\"Sentiment [-1,1]\")\n",
    "        ax21.set_ylabel(\"# of articles\")\n",
    "        ax21.set_title(\"[All] Sentiment Distribution\")\n",
    "        ax21.axvline(0, linestyle='--', linewidth=1)\n",
    "        if not np.isnan(avg_sent):\n",
    "            ax21.axvline(avg_sent, linestyle='-', linewidth=2)\n",
    "        ax21.text(0.98, 0.95,\n",
    "                  (f\"μ={avg_sent:.3f}\\nσ={std_sent:.3f}\"),\n",
    "                  ha='right', va='top', transform=ax21.transAxes, fontsize=10,\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.1))\n",
    "    else:\n",
    "        ax21.text(0.5, 0.5, \"No sentiment data in JSON\", ha='center', va='center')\n",
    "        ax21.set_title(\"[All] Sentiment Distribution\")\n",
    "        ax21.set_xticks([]); ax21.set_yticks([])\n",
    "\n",
    "    # (2,2) NEW: cumulative log author count histogram\n",
    "    _plot_log_author_hist(ax22, counts_per_author, \"[All]\")\n",
    "\n",
    "    fig.suptitle(\"All Sources — Distributions Panel\", fontsize=14)  # (kept)\n",
    "    fig.savefig(os.path.join(save_dir, \"all_sources_panel.png\"), dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    all_data: List[Tuple[NewsSource, pd.DataFrame]] = []\n",
    "\n",
    "    for i, source in enumerate(news_sources):\n",
    "        print(f\"Processing {source.key}... {i+1}/{len(news_sources)}\")\n",
    "        try:\n",
    "            df_src = load_source_df(source.key)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ⚠️ Missing file: {source.key}_articles.json — skipping.\")\n",
    "            # Create an empty frame with a sentiment column so downstream code is safe\n",
    "            df_src = pd.DataFrame({'sentiment': []})\n",
    "        all_data.append((source, df_src))\n",
    "        make_panel_for_source(source, df_src, FIG_DIR)\n",
    "\n",
    "    make_panel_cumulative(all_data, FIG_DIR)\n",
    "    print(f\"✅ Done. Saved {len(news_sources)} per-source panels + 1 cumulative panel → {os.path.abspath(FIG_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd4cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
